2018_06_26_0_meaningless_predictions.json: I tried to use the banded Fourier transform to distinguish between the background runs and the calibration runs. I found it to perform poorly, and also that the AP as it is is useless. Some further cuts are almost certainly required.
2018_06_26_1_perfect_ap_simulation.json: I added a bad attempt at an AP cut. The network just ended up simulating the AP function (quite well, in fact) but since the AP was determined in the last experiment to be useless at the moment, there is nothing meaningful here.
2018_06_29_improved_bubble_number_cut.json: There seems to be light at the end of the tunnel. I replaced the image-based bubble number filter with one based on the pressure transducer. After this cut, the data, while still quite noisy, has a much clearer separation between neutrons and alphas in both the neural network and AP axes.
2018_06_29_first_proper_fiducial_cuts.json: I added the original fiducial cuts used in previous research. It is starting to look much more reasonable; there are many more neutrons than alphas, but they are distinguished into a blob on the AP axis, outliers besides. However, the neural network's outputs are almost all less than 0.5 and some even go negative.
2018_06_29_improved_fiducial_cuts_with_sigmoid.json: I added a sigmoid activation on the final layer to prevent the negative results. It now looks pretty near the graph that was presented in the paper! The alphas are vastly outnumbered by neutrons. There are still neutron outliers, and over half of them are classified as alphas. That said, almost all of the alphas are classified as alphas. Not a perfect result, but it closely resembles what is shown in the PICO-60 paper.
2018_06_30_audio_classification_with_saved_validation_set.json: This was a major failure. The apparently high accuracy turned out to be the result of the network precisely outputting the average of all of the ground truths. Every one of the examples was classified with the same output, around 0.826. This brought to my attention a key problem with the network architecture: there are not enough convolutional layers. The first dense layer has well over five million parameters, which is vastly more than all of the other layers. It is bound to not learn properly on a comparatively small data set.
2018_06_30_audio_classification_fully_convolutional_architecture.json: This was a striking success. Using a fully convolutional architecture with no dense layers up to the very end, it was able to distinguish between neutrons and alphas with ~92% accuracy on the validation set. This was not as high as the performance got; it reached ~96% in the log but the validation data for that epoch was not saved. Most significantly, it reached that degree of accuracy with very little apparent correlation to the AP score. There were many decisive alphas and many decisive neutrons, and a significant number of outliers with a totally different pattern to that seen in the banded Fourier transform data. They were spread across the AP axis, and may potentially represent particles which are incorrectly classified in the (admittedly rough and imperfect) ground truth data set.
2018_06_30_audio_classification_with_saved_models: These validation sets, saved at each epoch, showed a validation performance which well exceeded that represented in the original PICO-60 paper. The neutrons and alpha particles were very clearly separated, with only a very small number of outliers in between. It is particularly notable in this data the degree of independence of the AP axis. There was effectively no correlation between the neural network's output and the AP score, making it clear that the network was cueing on a completely different, and presently unknown, characteristic of the audio recordings.
2018_06_30_audio_classification_with_saved_models_training_set_included.json: This was a test of the same system as in the previous record, but on the training set instead of the validation set. (It is a random sample of the combined training and validation sets, so a small amount of validation data may be included.) It represented absolutely perfect separation between the neutrons and alphas except for two incorrectly-classified (or supposedly incorrectly classified, since the training data is imperfect) alpha particles. It is striking the degree to which there is a near-perfect line at 0 and a near-perfect line at 1.
2018_07_05_audio_classification_including_all_calibration_sets: This validation data showed a graph much more rough and imprecise than the previous run including only americium-beryllium data. It was reasonably accurate at placing neutrons near the bottom of the grid, but the alphas were very inaccurately destributed throughout the neural network prediction axis, to a similar degree as it was for the AP axis.
2018_07_05_audio_classification_with_fiducial_cuts: This validation data showed neutrons classified relatively close to the 0 line. A non-trivial quantity of alphas were classified accurately, but nowhere near all of them. In fact, this experiment demonstrated a near-complete disregard for the state of the alphas. It was clear the neutrons were weighted much more heavily, since there are many more of them.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy: This was a disaster. The results varied widely between predicting almost everything as 0, to scattering the neutrons and alphas more or less randomly across the spectrum. It is not clear whether this is because of the class weights, or the binary crossentropy loss function which has caused problems previously.
2018_07_05_audio_classification_with_mean_squared_error: This was almost exactly the same as the previous run. The use of a mean squared error loss function rather than binary crossentropy seemed to make almost no difference. A major difference was observed in some previous tests, but not this time.
2018_07_06_audio_classification_with_only_californium_60cm: This was very similar to previous runs including all neutron sources. It tended to cluster all examples near 1 rather than 0, but this changed significantly between epochs. It was also observed to distribute examples throughout the spectrum. It was similarly unsuccessful to previous tests.
2018_07_06_audio_classification_with_only_barium_40cm: This had, once again, almost the exact same result. Examples were distributed evenly across the spectrum. There was almost no distinction between this run, californium at 60cm, and the mix of all classes.
2018_07_06_audio_classification_americium_without_pressure_cut: Performance on this test was not nearly as good as indicated in the training logs. Neutrons were vastly outnumbered by alphas, evidently as a result of the removal of the pressure cut. The 99.2% appeared to be a result of the vast majority of alphas being near 1; the neutrons were not especially consistent. This test raised more questions than it answered: why are there so many multi-bubble events in the low background set that had to be removed? Are they alphas being misclassified as multi-neutron events, or are they neutrons? If they are, there must be many single-neutron events in the low background data set as well.
2018_07_06_audio_classification_all_classes_very_deep_architecture: This was a surprising and promising result. Validation accuracy was fairly poor, because for many epochs, almost all examples were predicted as being above the 0.5 line. However, particularly in epochs 17 and 18, there was a very clear division between alpha particles and neutrons; it was very accurate, in fact. The line was just at the wrong location, well above 0.5. This provides some strong evidence that it is in fact possible to accurately discriminate, even when whatever bias was in the AmBe data set has been removed.
2018_07_07_audio_classification_all_classes_very_deep_architecture_training_set_included.json: This was a test of the very deep architecture on its training data set. It demonstrated almost the exact same line of separation (~0.9) as the validation data, demonstrating that this clear distinction is not a fluke. The trained network for epoch 18 (the second-last epoch) was used for evaluation.
2018_07_07_audio_classification_all_classes_very_deep_with_dense: The modified architecture including a dense layer shows promise. Most epochs demonstrated poor performance reminiscent of overfitting, with the exception of epoch 15, which proved a very effective discriminator. Its standard deviation is significantly better than that of AP on the data without fiducial cuts. Most bubbles cluster near 0 and 1, with a small number of outliers (which may plausibly represent incorrect ground truths).
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01: It is very evident from these validation sets that the regularization lambda parameter of 0.01 is very excessive. The behavior changes drastically between epochs, but it ranges from a meaningless cloud of points to a collection of all points at a specific average line. All models would be essentially useless as a discriminator.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001: The regularization lambda seems to still be a bit too high. There were some adequately accurate discriminators present within the various epochs, but none of them were decisive. They all resulted in a fairly wide cloud of outputs with a large standard deviation. Despite this, the best model managed ~91% accuracy.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003: This was the best model seen so far. It seemed to be a nearly optimal balance of the decisiveness of an unregularized model and the skepticism of a regularized model. Except for a single-digit number of outliers, bubbles were divided into defined lines at 0 and 1. Importantly, these lines were not composed only of the ground truth class belonging to that line. Some suspected alphas were classified decisively as neutrons and vice versa, indicating that the moderate degree of regularization gave the model freedom to ignore these examples rather than overfit to them. Once again, it was still sporadic and there were many bad epochs, but the good epochs (particularly 15) demonstrated apparently nearly ideal performance for this data set.
2018_07_09_audio_classification_with_zero_image_bubble_events: Performance here was comparable to previous tests but not ideal. Epoch 17 (with ~93% validation accuracy) had a large number of outliers not definitively classified into either of the two classes, although most of these were on the right side.. Epoch 15 (with ~91% accuracy) was more decisive, if apparently less accurate. Once again, there were epochs with completely useless performance, including at least one (epoch 19) which confidently classified almost every example as a neutron.
2018_07_09_audio_classification_with_cropped_inputs: Performance was very poor, even on the first epochs. The 81% accuracy statistic amounted to a rough cluster of neutrons at the bottom, a few sparse alphas at the top, and a large number of seemingly randomly placed bubbles across the graph. This effectively confirms the fear that there is some kind of bias in the noise at the beginning or the hydraulic sounds at the end of the recording, which can be used to discriminate.
2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization: This was certainly an improvement on the previous test, if not ideal. In the graph for epoch 16, the alphas were reasonably pure. Almost all of the neutrons were classified correctly. There was a significant number of alpha outliers confidently placed in neutron territory, enough that the standard deviation for that set of bubbles was higher than that for the AP discriminator.
2018_07_09_audio_classification_with_residual_blocks_on_every_convolutional_layer: As indicated in the training log, this run was remarkably stable. Performance was not good, and the graphs demonstrated very poor discrimination, but the network was consistently decisive (there were no aimless clouds) and its decisions remained almost the same on various epochs throughout the training run.
2018_07_10_audio_classification_with_more_layers_and_residual_blocks_on_every_two: The graphs for this test were surprising. The network showed a very extreme degree of decisiveness. There was a line at 0 and a line at 1, with no lack of confidence. Unfortunately, these lines were not accurate classifications. In epoch 10, a significant mass of neutrons (interestingly, clustered together in AP as well as network outputs) were confidently classified as alphas. However, the discrimination for epoch 3 was reasonably accurate; not perfect, but not terribly far off. However, it is questionable whether or not this degree of confidence is desirable, when so many are classified incorrectly.
2018_07_10_audio_classification_with_more_layers_and_no_residual_blocks: This was very similar to the previous run. It was very confident but frequently incorrect. The 73% accuracy peak is completely unacceptable for any kind of meaningful discrimination.
2018_07_10_audio_classification_validation_with_fiducial_cuts: This was a much more interesting and successful test. Reversion to the old network architecture improved things substantially. Validation performance on the set with fiducial cuts applied was very consistent; the graphs demonstrated reasonably accurate classification on the neutrons. The alphas were more questionable; many of them were on the wrong side. It is possible biases in the full data set are disturbing performance on the subset that passes fiducial cuts.
2018_07_10_audio_classification_trained_with_fiducial_cuts: After the recent demonstrations of strong stability in training and validation accuracy, this was a return to the rapid oscillations and inconsistent performance observed previously. It may be because of the smaller amount of training data, and the network finding new and creative ways to overfit on the dataset with each passing epoch. One epoch showed all of the bubbles clustered at 0, and one showed the neutrons mostly classified correctly but the alphas mostly disregarded.
2018_07_10_initial_high_resolution_frequency_domain: The graphs of this experiment are very simplistic and contain no meaningful information. Frequently everything is classified as a perfect 0 (neutron), or everything except for a small number of apparently memorized examples (which are sometimes incorrect themselves).
2018_07_10_audio_classification_with_secondary_position_input: Interestingly, the errors in this case are very one-sided. Almost all neutrons are classified correctly, but alphas are very spread out (in many cases, half of them are classified as neutrons). It is curious that neutrons seem to generalize so well to the validation set, but not alphas.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.0003: This was a very interesting result. It is very accurate at classifying neutrons which are in the usual (low) AP range, but is completely inaccurate for both neutrons and alphas in the higher AP range. This is similar to what was seen with the neural network in the PICO-60 paper. Also, now the inputs are fundamentally the same as those for the AP with positional correction (the position and the waveform) it is possible for the network to fully simulate the AP. It is not clear why this is the result of the network successfully optimizing its loss; this is clearly not what it is doing on the training data, as the accuracy is much higher on that set.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.001: This showed a very similar result, producing a validation set graph that looks very much like the neural network seen in the paper. The increased regularization seems to have made very little difference to the validation result.
2018_07_12_audio_classification_with_l2_0.0003_and_no_barium_data: This shows, once again, a strong correlation between the AP and the neural network's output. It is worth observing that while the neural network is not much better than the AP (although its standard deviation is slightly lower on both neutrons and alphas), the AP itself does not perform well on this data set. AP is supposedly able to achieve around 98% accuracy after many cuts, and it seems the next logical step for the neural network is to replicate those results with the AP and test on the same set, to see if its performance scales in the same way as that of AP.
2018_07_15_banded_frequency_new_validation_cuts.json: This was a re-test of the banded frequency training network after the new cuts on validation data. It shows that, in fact, even this very simple network performs worse than AP. After the new cuts, AP is a much more effective discriminator and shows a very small number of outliers. Comparatively, the neural network performs poorly, particularly on neutrons where it classifies a very large number of events as alphas.
2018_07_15_banded_frequency_smaller_architecture.json: While this is an improvement on the accuracy of the previous model, its decision boundary on the validation set is still very nebulous and indecisive.
2018_07_16_banded_frequency_validation_not_removed.json: This was much more successful than the preceding run. The accuracy of the neural network for discrimination was similar to that of AP, and the standard deviation was lower for both classes. The network is not entirely decisive, but its architecture is very simple so this is not overly surprising.
2018_07_16_banded_frequency_training_data_passes_validation_cuts.json: The graphs from this experiment are much more decisive, and there is a wider decision boundary between alphas and neutrons. There is clearly a correlation between AP and the network's outputs, since the very high-AP neutrons are classified incorrectly much more often by the neural network as compared to the lower-AP neutrons which still intersect with the alpha AP range. This was the case in the previous experiment as well, but it is interesting that it is still very evident even when the decision boundary is clearer.
2018_07_16_banded_frequency_linear_regression.json: This has a characteristically different decision boundary as compared to the neural network. It is much more similar to the AP, with a less defined separation. A sigmoidal curve can be seen in the lower-AP alphas, where they are brought closer to the neutron range presumably so that the neutrons in the lower end of the alpha range can be classified correctly. It is slightly more accurate than the AP, but it does not have the desirable decision boundary that seems to be more attainable with a neural network.
2018_07_16_banded_frequency_validation_passing_weighted_3_times.json: This is further evidence of the correlation, if not equality, between the neural network's outputs and AP. It visually looks like a distorted version of AP such that it predicts almost all examples correctly, even though its boundary is not particularly decisive. There still does not appear to be enough information in the banded frequency domain data to accurately pick out the high-AP neutron events.
2018_07_16_banded_frequency_weight_reduced_to_2_times.json: The reduction of weight on the validation test passing data does not appear to have improved anything. More interestingly, the validation set in this instance contains more outliers on the AP axis. This makes the graph clearly demonstrate the inability of the network to separate its outputs from differences on the AP axis. It is a distortion, and if there are a large number of alphas mixed in with the higher-AP neutrons, they will be classified incorrectly.
2018_07_16_waveform_with_new_cuts: In the validation data for this test, the neural network is almost completely useless. The neutrons and alphas are distributed far and wide through the output axis, and one could not make any kind of meaningful prediction based on it. The AP is far, far more accurate. This strongly indicates two important results. One, that all information necessary for significant overfitting has been removed. There was no noise or other bias that the network could use. Two, that without the position corrections, it is impossible to distinguish between neutrons and alphas, at least using the raw waveform. This is not surprising, but there was no existing evidence that this was the case. It is known to be possible to distinguish using the volume, and this indicates that this is absolutely necessary.
2018_07_16_waveform_with_new_cuts_and_position_input: There is little of note to be seen in this experiment. Its performance is poor and tremendously unstable, with its predictions changing drastically between epochs. However, it is interesting that it is not better than the previous experiment. All information encapsulated in the 8-band frequency domain data is in the inputs to this network, but it is not learning how to use it. As such, this contradicts the evidence that the previous experiment failed because the inputs were not sufficient. Perhaps the raw waveform is not an ideal format, and it must be further processed (such as through a Fourier transform).
2018_07_17_banded_frequency_normalized: The 89% accuracy statistic is somewhat misleading. From the graph of the network's decision boundary, it is clearly far from confident. Its outputs form a vague correlation at most with the actual status of the examples. Clearly, the removed information was very important.
2018_07_17_banded_frequency_not_position_corrected: Very high discrimination accuracy was observed in this test. The graph is not perfect and there are some outliers, suggesting that it was not overfitting excessively. Indeed, this appears to be an exceptional validation set, but the network is just as accurate as AP. And, of course, it is significant that this was achieved without any kind of position correction or even position input.
2018_07_17_banded_frequency_not_position_corrected_retest: Performance here was seemingly no worse than AP. The neural network got just as many right, except that its distribution was less one-sided than AP. Where AP classified several neutrons in alpha territory, the network struck more of a balance in a way that could not be achieved through a simple transformation on the AP function.
2018_07_17_banded_frequency_not_position_corrected_with_position_input: Performance in this epoch was meaningfully better than AP, especially at the peaks of validation performance. It is also better than the performance of a network trained on the banded frequency domain data without the position of the bubble. It managed to correctly classify several examples on which AP failed. This is a strong indication that the network is gaining information from the positional data.
2018_07_17_banded_frequency_custom_fourier_transform: The networks trained here are essentially useless as discriminators. This is not unexpected, as the frequency bands used are almost certainly incorrect.
2018_07_18_banded_frequency_no_dropout: The graphs for this test showed a standard deviation for alpha particles significantly higher than AP, but lower for neutrons. Subjectively, it was not entirely accurate at picking neutrons out of the more mixed regions of the AP axis. This may be a result of overfitting on the training set.
2018_07_18_banded_frequency_25%_dropout: The standard deviations for both classes are better for the network's outputs than AP. Also, it is subjectively very accurate and reasonable in its decisions, with a fairly small number of outliers. This is the best result seen so far with the banded frequency network.
2018_07_18_iterative_cluster_nucleation_banded: The graph results for this technique are not bad per se, but they are very scattered and imprecise. There is a large amount of overlap between the neutrons and alphas. It seems somewhat reminiscent of excessive regularization. That is a difficult balance to strike for this task, since lower regularization will result in more incorrect examples being added to the ground truth set.
2018_07_18_iterative_cluster_nucleation_with_l2_0.001: On this training run, the network was never as decisive as when it was trained without iterative cluster nucleation. However, it still managed to obtain higher accuracy than AP, as well as a lower standard deviation for both classes.
2018_07_18_iterative_cluster_nucleation_increasing_threshold: Once again, there were some epochs (such as 71) that attained higher accuracy than AP as well as a lower standard deviation for both classes. This was an unremarkable change from the previous test.
2018_07_18_iterative_cluster_nucleation_with_256_initial_examples: This was very successful! While the validation set is low-noise, there are epochs when it had higher accuracy, and certainly lower standard deviations for both classes. The choices on the validation set were not as decisive as with the full training set, but were nonetheless very accurate.
2018_07_18_iterative_cluster_nucleation_without_validation_cuts: In the better-performing epochs during this run, accuracy was frequently similar to or worse than AP, even though AP is significantly less accurate on this expanded data set. That said, the standard deviation was usually similar or lower.
2018_07_18_iterative_cluster_nucleation_with_increased_regularization: Performance as measured by the graphs from this test was quite poor. The decision boundary was extremely unclear, and there was a huge amount of overlap even though AP was reasonably accurate for this data set. This provides some evidence that the low-resolution banded Fourier transform is not particularly effective for data without fiducial cuts, possibly due to the very different acoustics.
2018_07_18_iterative_cluster_nucleation_removed_second_last_layer: Performance was very poor, once again. The neutrons were sometimes reasonably clustered, but the alphas were wildly scattered across the graph.
2018_07_18_banded_frequency_no_validation_cuts_at_all: Despite the poor performance, it is in fact better than AP on the validation set. This is true, for certain epochs, in accuracy as well as standard deviation for both classes. It is by no means precise; there are outliers scattered across the graph. But this is true even more so for AP than for the neural network's output.
2018_07_19_iterative_cluster_nucleation_with_128_initial_examples: AP is very accurate indeed for this validation set. For some reason, the neural network performs poorly and seems to overfit extensively. The neutrons which are clustered together on the AP axis are classified reasonably accurately, but the alphas are scattered all over to the map, to the point where the network's outputs are almost meaningless. This may be a result of extensive overfitting.
2018_07_19_gravitational_first_test: For this run, two different data sets are saved per epoch. One that shows conventional validation accuracy, and one that compares the dynamically computed gravitational ground truths to the original, correct ground truths for the training set. The validation graphs showed rather generic poor performance, with nothing particularly notable. The training ground truth graphs showed an essentially random, meaningless split between the examples that happened to end up on the top of the graph versus those that happened to end up on the bottom, where the examples would, through overfitting, stay wherever they were initially, for the most part. This indicates that the original, correct training data does not have nearly enough influence over the original training process.
2018_07_19_gravitational_offset: It appears that the relatively small influence of the gravity means that it disturbs the training process somewhat but does not result in meaningfully confident classifications. Throughout the training run, neutrons were often reasonably clustered in the bottom left (not neutron AP outliers, though), while alphas were scattered all over the graph on both axes.
2018_07_19_banded_frequency_signal_time_bin_only_and_no_regularization: Removal of the unnecessary data and all regularization did not prevent the neural network from matching the performance of AP. It produced equivalent accuracy (with a closer to equal distribution of outliers) and a lower standard deviation for both classes.
2018_07_20_custom_fourier_transform: Surprisingly, training was actually quite successful! While accuracy was not quite as accurate as AP (and this could be a quirk of the validation set) the standard deviation was lower for both classes. This demonstrates that nothing too important was destroyed when making the custom Fourier transform (although it is only 5 bands rather than 8).
2018_07_20_divide_by_squared_number_of_samples: Once again, while accuracy is not bad, it is worse than AP. (The division seems to have made very little difference.) That indicates that perhaps the number of bands does make a difference. At the moment, I do not have access to the exact bands used, but I should be able to increase the resolution within the same overall range, and performance may increase.
2018_07_20_more_bands: The validation set happened to be very clean for this run, so its high accuracy is not very meaningful. It is no better than AP. A re-test is required.
2018_07_20_more_bands_retest: The validation set was indeed less clean. While the network attained lower standard deviations compared to AP, its accuracy was the same at the very best.
2018_07_20_20_bands: Finally, the custom Fourier transform produced quantitatively better accuracy than AP! It also managed lower standard deviations for both classes. It seems that, in general, adding more bands improves performance. Perhaps this should be taken further.
2018_07_20_40_bands: Once again, it was better than AP, but slightly worse than the previous test. This may be an indication that higher resolutions no longer improve performance, or it may simply represent overfitting that can be alleviated with further regularization (beyond the 50% dropout that is currently being used).
2018_07_20_40_bands_tanh_activation: There was much more to this change than originally met the eye. In the later epochs, it was much more decisive, with more examples very close to 0 and 1. It was also still more accurate than AP, with a significant difference visible at the edge of the alphas on the AP axis.
2018_07_20_l2_0.003: This was successful! The validation accuracy was ~93%, but the validation set was not clean. It classified several examples correctly that AP did not. Its standard deviation on neutrons was lower than AP, but similar for alphas.
2018_07_20_20_bands_no_regularization: A stupendous result can be viewed at epoch 237. On that particular validation set, AP is almost useless. Yet the neural network is quite accurate at discriminating between neutrons and alphas in the same AP ranges. Not only is its accuracy greater, its standard deviations for both classes are significantly lower. This is a very positive indication indeed that the higher Fourier transform resolution helps performance.
2018_07_20_no_wall_cuts: This was a strange result. There was an unexpectedly clear separation along the AP axis compared to previous experiments. The neural network performed reasonably, but not even as well as AP in terms of accuracy. (Standard deviations for both classes were still better.) It is worth re-testing this on a hopefully more representative validation set.
2018_07_20_no_wall_cuts_retest: There were neutrons and alphas scattered all over the graph on both axes. The network's accuracy was somewhat similar to that of AP, but it is difficult to say whether or not it was better. Neither was particularly effective. One thing that was very clear for this test, but was also observed in previous experiments, is that, for detecting neutrons, the neural network has a fairly high precision but a lower recall. This means there are several neutrons misclassified as alphas, but fewer alphas misclassified as neutrons. The reason for this is not entirely clear. Multi-bubble events are filtered out of the data set, so they are unlikely to be the cause.
2018_07_21_full_resolution_fourier_transform: This was extremely successful! Epoch 213 demonstrated not only a very accurate, but extremely decisive decision boundary between neutrons and alphas. The standard deviations for both classes are much lower, and there are less than half as many errors compared to AP.
2018_07_21_dropout_0.5: The 50% dropout clearly made it fit worse to the training and validation data. Its accuracy was still slightly better than AP, and its standard deviations were lower, but it was significantly worse at making precise decisions independent of AP. Performance was closer to what was observed with the 8-band Fourier transform.
2018_07_21_l2_0.003: The graph for this run was notably less decisive. Examples were scattered over the graph to a much greater degree. Accuracy was still slightly better than AP, but not nearly as evidently. This may be an indication that regularization is not beneficial for this network architecture, which is very simple and has few neurons for all layers except the first.
2018_07_21_iterative_cluster_nucleation_full_resolution_without_wall_cuts: Interestingly, some of the epochs at the beginning demonstrated a semi-reasonable (if not good) decision boundary. For most of the later part of the run, as more incorrect examples were added to the training set, it classified the entire validation set confidently as alphas, except for two alphas which were classified as neutrons. This is probably explicable by overfitting.
2018_07_24_iterative_cluster_nucleation_banded_frequency_no_duplication: This result was unremarkable. Predictions were distributed around the graph. It is likely due to the lack of validation wall cuts in combination with the banded frequency training system, which has never worked well.
2018_07_24_iterative_cluster_nucleation_with_wall_cuts: The decision boundary was very vague and cloudy at the beginning. Later on, it clearly began to overfit, classifying many of the alphas confidently as neutrons.
2018_07_24_iterative_cluster_nucleation_with_dropout_and_l2: Performance here was slightly worse than AP. It got most examples right that AP got right, but didn't pick out any neutron AP outliers in the way the conventionally trained system did. It seems that the process of expanding the training set included neutrons that were in the neutron cluster, but not those that were in the alpha cluster. This is not surprisingly, but is a severe limitation that makes iterative cluster nucleation far less useful.
2018_07_24_high_resolution_frequency_grid_search: There was very good performance indeed observed in this grid search! Due to the lack of wall cuts, AP was questionably useful, and there was a great degree of overlap. However, the network managed vastly better performance in certain epochs, discriminating very accurately even when both particles are within the same AP range. Admittedly these were outnumbered by the runs that showed mediocre accuracy, but this could be due to the noise inherent in training with relatively small data sets.
2018_07_26_image_classification_less_dropout_more_epochs: As it turns out, AP is not an accurate discriminator for this particular validation set. This means that the network's performance is actually not very different. However, this does not translate to a confident judgment about whether or not this technique works. The network is certainly not accurate; its predictions seem to be scattered, almost at random.
2018_07_26_image_classification_fixed_loading: Even on the epochs with the best accuracy on the validation set, the network's predictions appear almost random, with a slight bias in favor of the right answer. Standard deviations on both classes are nearly the same as random numbers.
2018_07_27_image_classification_with_l2: The network's predictions are scattered more or less randomly across the graph, as would be expected from an accuracy that low.
2018_07_27_iterative_cluster_nucleation_grid_search: The better-performing models were generally very successful, and significantly better than AP in terms of accuracy. The standard deviations were frequently similar or higher, but in some cases AP was outperformed in that respect. The results were all over the map. On the better-performing models, a similar decision boundary to the supervised learning system was observed. The low-AP neutrons and high-AP alphas were classified very accurately and confidently, whereas the ones near the boundary were more questionable. In some cases, they were still classified accurately, if not as decisively.
2018_07_28_image_classification_l2_lambda_0.003: Once again, the network's predictions are very close to random. It does not seem to be able to extract anything useful from this dataset, at least with this architecture.
2018_07_28_image_classification_more_convolutional_layers: A very strange result indeed. For many of the later epochs, the network predicted every example to be at the average. At the beginning, however, a very bizarre decision boundary was observed, with two lines at which the examples are clustered (with no particular regard to being correct) and examples scattered outside these lines, but almost nothing between them. It is not clear at all what causes this behavior to arise. However, it is notable that there are many examples predicted outside the range of 0 to 1, because there is no activation on the last layer.
2018_07_28_image_classification_sigmoid_last_layer: This was somewhere between the last two tests. At the very beginning, the predictions were randomly scattered. But, later on, they were once again on an average line. There seems to be no correlation between the inputs and outputs. However, a grid search may clear this up.
2018_07_28_waveform_grid_search: The results were not as bad as they seemed to be based on the training log. There was at least one saved validation set (lambda=0.003, dropout=0, first layer filters=24, kernel size=3, convolutional layers per group=3, epoch=78) that had standard deviations on both classes lower than AP. Its accuracy was also better; AP did not perform well on that set. However, most other results were worse, and none of the accuracy values compared favorably to the networks trained on Fourier transforms, so this technique is worse overall.
2018_07_29_image_grid_search: The graphs did not demonstrate any meaningful ability to discriminate. In the best-performing epoch overall (run 0), the points were clustered into two lines, one around 0.2 and one around 0.5, with few outliers. It is not clear why the clusters occurred at those specific positions. Also, the clusters had almost no correlation to the ground truth values. On run 1, similar clusters were observed, but with more noise. In any case, the graphs do not show any kind of promise for this technique.
2018_07_30_gravitational_stochastic_gradient_descent: It seems that the gravitational modifications are not having enough effect. Even at the end of the training run, all examples are fairly scattered over the range of around 0.2 to 0.5. Also, there is no separation between the neutrons and alphas; the small number of definitive ground truths seem to do nothing. It may require a multiplier of 0 at the beginning and a larger learning rate so the minority of examples that are definitive have a stronger effect.
2018_07_30_gravitational_increasing_gravity_multiplier: It is still not learning well enough. Both the ground truths and validation predictions are distributed across the spectrum, even at the end. Perhaps the relatively small number of definitive examples, combined with the few non-definitive examples that are located close enough to the edges of the spectrum to have a significant effect on training, are outweighed by the inert bulk of examples that remain in the same place.
2018_07_30_gravitational_gravity_multiplier_increment_0.003: At the end, there was still a very large amount of overlap in the graph of gravitational ground truths. It is possible the clusters of alphas and neutrons were simultaneously being pulled in both directions due to their straddling of the 0.5 line. However, the validation graph demonstrated fairly good (if not particularly decisive) accuracy. Its standard deviation for neutrons on the last epoch was even better than AP (but not for alphas).
2018_08_01_ap_simulation: Performance was actually reasonable; there was a nearly linear correlation between the network's output and AP. Still, there was a fairly large amount of error on both sides of the line.
2018_08_01_ap_simulation_no_regularization: As expected, the spread is somewhat tighter. It is still far from a perfect replication of AP.
2018_08_01_position_corrected_input: The clustering is vastly tighter; it may not be possible to achieve much better than this using a neural network with a sane architecture.
2018_08_01_raw_audio_no_position_input: The graph looks very similar to how it did with no position corrections but with position input. There is certainly more variance compared to the last test, but it is nevertheless mostly reasonably accurate.
2018_08_01_custom_fft: Contradicting what appeared to be reasonable in the training log, the network's predictions were completely inane. There was a cluster around (0, 0), and then the network's predictions seemed to be almost random. It is interesting that the mean absolute error is so similar when subjectively the performance is so much worse.
2018_08_01_custom_fft_l2_lambda_0.06: The network was significantly better at simulating AP than on the last experiment. Its outputs were certainly more closely correlated. However, it was not exactly linear. Despite not having any activation on the last layer, the outputs took on an shape not unlike a binary classifier. There were clusters in the bottom left and top right, and a very steep slope in the middle with rather less correlation to AP.
2018_08_02_iterative_cluster_nucleation_best_hyperparameters_from_grid_search: The closest this model ever agreed with AP was 9 disagreements, which corresponds to ~93% accuracy. Visually, it interpreted a few of the alphas as neutrons, much like how the initial set is classified. This confirms that the extremely good performance during the grid search was a fluke.
2018_08_13_triplets_nucleation_grid_search_saved_validation_sets: Analysis of class-wise standard deviations revealed further information about this grid search. The overall mean in the first run (which had the best overall accuracy) was 0.18, which is excellent compared to the 0.49 for AP on the same set. In addition, the Pearson correlation between training and validation accuracy was calculated; it is fairly weak, at 0.3517387044146793.
2018_08_28_pulse_count_saving_validation: Predictions on the validation set are very confident indeed, with classifications nearly equal to 0 and 1. There are a small number of events scattered across the middle of the graph.
2018_08_28_pulse_count_80_to_240: Classifications in this test were very similar to the previous one, with no notable changes due to the cut on the number of PMTs.
2018_08_29_real_world_test_data: Almost the entire real-world set of events were clustered near 0, with fairly high confidence. There are a very small number present in the middle of the graph.
2018_08_29_real_world_timings_input: As expected from the accuracy statistics, the network's degree of decisiveness changed very little.
2018_08_29_real_world_timings_input_more_layers: With more layers, the network became much more decisive, with the amount of spread for each class decreasing to very nearly 0 for real world and validation samples (although more extreme for validation data).
2018_08_29_real_world_timings_input_only: Using only timings information, the validation set was still classified with nearly absolute confidence, while the spread on the real world data increased somewhat.
2018_08_30_real_world_linear_regression: The distribution was interesting and unexpected. It was not as simple as predicting the same value for every example. Rather, the outputs appeared to be in the form of a Gaussian distribution. There was clearly a difference between the distributions of the two classes, but they almost entirely overlapped, so these distributions could not be used to make meaningful predictions.
2018_08_30_real_world_photon_count_cut: It seems confidence has increased somewhat, with the cluster of predicted recoils very tightly packed and the predicted neck alphas further to the right, between 0.8 and 0.9.
2018_09_28_map_projection: The network's predictions are absolutely perfectly confident, in the resolution viewable in the histogram. It seems to be even more confident than most fully connected network models.
2018_09_28_map_projection_1_conv_4_filters: It is very obvious here that even in the better epochs, predictions are much less confident with a smaller network architecture. For the neck events in particular, there is a very wide Gaussian-like distribution that goes right down to 0.5.
2018_09_28_map_projection_with_real_world_test: Predictions on real-world data do not look tremendously different from before. Most events are confidently predicted to be nuclear recoils, with a few scattered around the prediction space.
