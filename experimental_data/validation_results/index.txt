2018_06_26_0_meaningless_predictions.json: I tried to use the banded Fourier transform to distinguish between the background runs and the calibration runs. I found it to perform poorly, and also that the AP as it is is useless. Some further cuts are almost certainly required.
2018_06_26_1_perfect_ap_simulation.json: I added a bad attempt at an AP cut. The network just ended up simulating the AP function (quite well, in fact) but since the AP was determined in the last experiment to be useless at the moment, there is nothing meaningful here.
2018_06_29_improved_bubble_number_cut.json: There seems to be light at the end of the tunnel. I replaced the image-based bubble number filter with one based on the pressure transducer. After this cut, the data, while still quite noisy, has a much clearer separation between neutrons and alphas in both the neural network and AP axes.
2018_06_29_first_proper_fiducial_cuts.json: I added the original fiducial cuts used in previous research. It is starting to look much more reasonable; there are many more neutrons than alphas, but they are distinguished into a blob on the AP axis, outliers besides. However, the neural network's outputs are almost all less than 0.5 and some even go negative.
2018_06_29_improved_fiducial_cuts_with_sigmoid.json: I added a sigmoid activation on the final layer to prevent the negative results. It now looks pretty near the graph that was presented in the paper! The alphas are vastly outnumbered by neutrons. There are still neutron outliers, and over half of them are classified as alphas. That said, almost all of the alphas are classified as alphas. Not a perfect result, but it closely resembles what is shown in the PICO-60 paper.
2018_06_30_audio_classification_with_saved_validation_set.json: This was a major failure. The apparently high accuracy turned out to be the result of the network precisely outputting the average of all of the ground truths. Every one of the examples was classified with the same output, around 0.826. This brought to my attention a key problem with the network architecture: there are not enough convolutional layers. The first dense layer has well over five million parameters, which is vastly more than all of the other layers. It is bound to not learn properly on a comparatively small data set.
2018_06_30_audio_classification_fully_convolutional_architecture.json: This was a striking success. Using a fully convolutional architecture with no dense layers up to the very end, it was able to distinguish between neutrons and alphas with ~92% accuracy on the validation set. This was not as high as the performance got; it reached ~96% in the log but the validation data for that epoch was not saved. Most significantly, it reached that degree of accuracy with very little apparent correlation to the AP score. There were many decisive alphas and many decisive neutrons, and a significant number of outliers with a totally different pattern to that seen in the banded Fourier transform data. They were spread across the AP axis, and may potentially represent particles which are incorrectly classified in the (admittedly rough and imperfect) ground truth data set.
2018_06_30_audio_classification_with_saved_models: These validation sets, saved at each epoch, showed a validation performance which well exceeded that represented in the original PICO-60 paper. The neutrons and alpha particles were very clearly separated, with only a very small number of outliers in between. It is particularly notable in this data the degree of independence of the AP axis. There was effectively no correlation between the neural network's output and the AP score, making it clear that the network was cueing on a completely different, and presently unknown, characteristic of the audio recordings.
2018_06_30_audio_classification_with_saved_models_training_set_included.json: This was a test of the same system as in the previous record, but on the training set instead of the validation set. (It is a random sample of the combined training and validation sets, so a small amount of validation data may be included.) It represented absolutely perfect separation between the neutrons and alphas except for two incorrectly-classified (or supposedly incorrectly classified, since the training data is imperfect) alpha particles. It is striking the degree to which there is a near-perfect line at 0 and a near-perfect line at 1.
2018_07_05_audio_classification_including_all_calibration_sets: This validation data showed a graph much more rough and imprecise than the previous run including only americium-beryllium data. It was reasonably accurate at placing neutrons near the bottom of the grid, but the alphas were very inaccurately destributed throughout the neural network prediction axis, to a similar degree as it was for the AP axis.
2018_07_05_audio_classification_with_fiducial_cuts: This validation data showed neutrons classified relatively close to the 0 line. A non-trivial quantity of alphas were classified accurately, but nowhere near all of them. In fact, this experiment demonstrated a near-complete disregard for the state of the alphas. It was clear the neutrons were weighted much more heavily, since there are many more of them.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy: This was a disaster. The results varied widely between predicting almost everything as 0, to scattering the neutrons and alphas more or less randomly across the spectrum. It is not clear whether this is because of the class weights, or the binary crossentropy loss function which has caused problems previously.
2018_07_05_audio_classification_with_mean_squared_error: This was almost exactly the same as the previous run. The use of a mean squared error loss function rather than binary crossentropy seemed to make almost no difference. A major difference was observed in some previous tests, but not this time.
2018_07_06_audio_classification_with_only_californium_60cm: This was very similar to previous runs including all neutron sources. It tended to cluster all examples near 1 rather than 0, but this changed significantly between epochs. It was also observed to distribute examples throughout the spectrum. It was similarly unsuccessful to previous tests.
2018_07_06_audio_classification_with_only_barium_40cm: This had, once again, almost the exact same result. Examples were distributed evenly across the spectrum. There was almost no distinction between this run, californium at 60cm, and the mix of all classes.
2018_07_06_audio_classification_americium_without_pressure_cut: Performance on this test was not nearly as good as indicated in the training logs. Neutrons were vastly outnumbered by alphas, evidently as a result of the removal of the pressure cut. The 99.2% appeared to be a result of the vast majority of alphas being near 1; the neutrons were not especially consistent. This test raised more questions than it answered: why are there so many multi-bubble events in the low background set that had to be removed? Are they alphas being misclassified as multi-neutron events, or are they neutrons? If they are, there must be many single-neutron events in the low background data set as well.
2018_07_06_audio_classification_all_classes_very_deep_architecture: This was a surprising and promising result. Validation accuracy was fairly poor, because for many epochs, almost all examples were predicted as being above the 0.5 line. However, particularly in epochs 17 and 18, there was a very clear division between alpha particles and neutrons; it was very accurate, in fact. The line was just at the wrong location, well above 0.5. This provides some strong evidence that it is in fact possible to accurately discriminate, even when whatever bias was in the AmBe data set has been removed.
2018_07_07_audio_classification_all_classes_very_deep_architecture_training_set_included.json: This was a test of the very deep architecture on its training data set. It demonstrated almost the exact same line of separation (~0.9) as the validation data, demonstrating that this clear distinction is not a fluke. The trained network for epoch 18 (the second-last epoch) was used for evaluation.
2018_07_07_audio_classification_all_classes_very_deep_with_dense: The modified architecture including a dense layer shows promise. Most epochs demonstrated poor performance reminiscent of overfitting, with the exception of epoch 15, which proved a very effective discriminator. Its standard deviation is significantly better than that of AP on the data without fiducial cuts. Most bubbles cluster near 0 and 1, with a small number of outliers (which may plausibly represent incorrect ground truths).
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01: It is very evident from these validation sets that the regularization lambda parameter of 0.01 is very excessive. The behavior changes drastically between epochs, but it ranges from a meaningless cloud of points to a collection of all points at a specific average line. All models would be essentially useless as a discriminator.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001: The regularization lambda seems to still be a bit too high. There were some adequately accurate discriminators present within the various epochs, but none of them were decisive. They all resulted in a fairly wide cloud of outputs with a large standard deviation. Despite this, the best model managed ~91% accuracy.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003: This was the best model seen so far. It seemed to be a nearly optimal balance of the decisiveness of an unregularized model and the skepticism of a regularized model. Except for a single-digit number of outliers, bubbles were divided into defined lines at 0 and 1. Importantly, these lines were not composed only of the ground truth class belonging to that line. Some suspected alphas were classified decisively as neutrons and vice versa, indicating that the moderate degree of regularization gave the model freedom to ignore these examples rather than overfit to them. Once again, it was still sporadic and there were many bad epochs, but the good epochs (particularly 15) demonstrated apparently nearly ideal performance for this data set.
2018_07_09_audio_classification_with_zero_image_bubble_events: Performance here was comparable to previous tests but not ideal. Epoch 17 (with ~93% validation accuracy) had a large number of outliers not definitively classified into either of the two classes, although most of these were on the right side.. Epoch 15 (with ~91% accuracy) was more decisive, if apparently less accurate. Once again, there were epochs with completely useless performance, including at least one (epoch 19) which confidently classified almost every example as a neutron.
2018_07_09_audio_classification_with_cropped_inputs: Performance was very poor, even on the first epochs. The 81% accuracy statistic amounted to a rough cluster of neutrons at the bottom, a few sparse alphas at the top, and a large number of seemingly randomly placed bubbles across the graph. This effectively confirms the fear that there is some kind of bias in the noise at the beginning or the hydraulic sounds at the end of the recording, which can be used to discriminate.
2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization: This was certainly an improvement on the previous test, if not ideal. In the graph for epoch 16, the alphas were reasonably pure. Almost all of the neutrons were classified correctly. There was a significant number of alpha outliers confidently placed in neutron territory, enough that the standard deviation for that set of bubbles was higher than that for the AP discriminator.
2018_07_09_audio_classification_with_residual_blocks_on_every_convolutional_layer: As indicated in the training log, this run was remarkably stable. Performance was not good, and the graphs demonstrated very poor discrimination, but the network was consistently decisive (there were no aimless clouds) and its decisions remained almost the same on various epochs throughout the training run.
2018_07_10_audio_classification_with_more_layers_and_residual_blocks_on_every_two: The graphs for this test were surprising. The network showed a very extreme degree of decisiveness. There was a line at 0 and a line at 1, with no lack of confidence. Unfortunately, these lines were not accurate classifications. In epoch 10, a significant mass of neutrons (interestingly, clustered together in AP as well as network outputs) were confidently classified as alphas. However, the discrimination for epoch 3 was reasonably accurate; not perfect, but not terribly far off. However, it is questionable whether or not this degree of confidence is desirable, when so many are classified incorrectly.
2018_07_10_audio_classification_with_more_layers_and_no_residual_blocks: This was very similar to the previous run. It was very confident but frequently incorrect. The 73% accuracy peak is completely unacceptable for any kind of meaningful discrimination.
2018_07_10_audio_classification_validation_with_fiducial_cuts: This was a much more interesting and successful test. Reversion to the old network architecture improved things substantially. Validation performance on the set with fiducial cuts applied was very consistent; the graphs demonstrated reasonably accurate classification on the neutrons. The alphas were more questionable; many of them were on the wrong side. It is possible biases in the full data set are disturbing performance on the subset that passes fiducial cuts.
2018_07_10_audio_classification_trained_with_fiducial_cuts: After the recent demonstrations of strong stability in training and validation accuracy, this was a return to the rapid oscillations and inconsistent performance observed previously. It may be because of the smaller amount of training data, and the network finding new and creative ways to overfit on the dataset with each passing epoch. One epoch showed all of the bubbles clustered at 0, and one showed the neutrons mostly classified correctly but the alphas mostly disregarded.
2018_07_10_initial_high_resolution_frequency_domain: The graphs of this experiment are very simplistic and contain no meaningful information. Frequently everything is classified as a perfect 0 (neutron), or everything except for a small number of apparently memorized examples (which are sometimes incorrect themselves).
2018_07_10_audio_classification_with_secondary_position_input: Interestingly, the errors in this case are very one-sided. Almost all neutrons are classified correctly, but alphas are very spread out (in many cases, half of them are classified as neutrons). It is curious that neutrons seem to generalize so well to the validation set, but not alphas.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.0003: This was a very interesting result. It is very accurate at classifying neutrons which are in the usual (low) AP range, but is completely inaccurate for both neutrons and alphas in the higher AP range. This is similar to what was seen with the neural network in the PICO-60 paper. Also, now the inputs are fundamentally the same as those for the AP with positional correction (the position and the waveform) it is possible for the network to fully simulate the AP. It is not clear why this is the result of the network successfully optimizing its loss; this is clearly not what it is doing on the training data, as the accuracy is much higher on that set.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.001: This showed a very similar result, producing a validation set graph that looks very much like the neural network seen in the paper. The increased regularization seems to have made very little difference to the validation result.
2018_07_12_audio_classification_with_l2_0.0003_and_no_barium_data: This shows, once again, a strong correlation between the AP and the neural network's output. It is worth observing that while the neural network is not much better than the AP (although its standard deviation is slightly lower on both neutrons and alphas), the AP itself does not perform well on this data set. AP is supposedly able to achieve around 98% accuracy after many cuts, and it seems the next logical step for the neural network is to replicate those results with the AP and test on the same set, to see if its performance scales in the same way as that of AP.
2018_07_15_banded_frequency_new_validation_cuts.json: This was a re-test of the banded frequency training network after the new cuts on validation data. It shows that, in fact, even this very simple network performs worse than AP. After the new cuts, AP is a much more effective discriminator and shows a very small number of outliers. Comparatively, the neural network performs poorly, particularly on neutrons where it classifies a very large number of events as alphas.
2018_07_15_banded_frequency_smaller_architecture.json: While this is an improvement on the accuracy of the previous model, its decision boundary on the validation set is still very nebulous and indecisive.
2018_07_16_banded_frequency_validation_not_removed.json: This was much more successful than the preceding run. The accuracy of the neural network for discrimination was similar to that of AP, and the standard deviation was lower for both classes. The network is not entirely decisive, but its architecture is very simple so this is not overly surprising.
2018_07_16_banded_frequency_training_data_passes_validation_cuts.json: The graphs from this experiment are much more decisive, and there is a wider decision boundary between alphas and neutrons. There is clearly a correlation between AP and the network's outputs, since the very high-AP neutrons are classified incorrectly much more often by the neural network as compared to the lower-AP neutrons which still intersect with the alpha AP range. This was the case in the previous experiment as well, but it is interesting that it is still very evident even when the decision boundary is clearer.
2018_07_16_banded_frequency_linear_regression.json: This has a characteristically different decision boundary as compared to the neural network. It is much more similar to the AP, with a less defined separation. A sigmoidal curve can be seen in the lower-AP alphas, where they are brought closer to the neutron range presumably so that the neutrons in the lower end of the alpha range can be classified correctly. It is slightly more accurate than the AP, but it does not have the desirable decision boundary that seems to be more attainable with a neural network.
2018_07_16_banded_frequency_validation_passing_weighted_3_times.json: This is further evidence of the correlation, if not equality, between the neural network's outputs and AP. It visually looks like a distorted version of AP such that it predicts almost all examples correctly, even though its boundary is not particularly decisive. There still does not appear to be enough information in the banded frequency domain data to accurately pick out the high-AP neutron events.
2018_07_16_banded_frequency_weight_reduced_to_2_times.json: The reduction of weight on the validation test passing data does not appear to have improved anything. More interestingly, the validation set in this instance contains more outliers on the AP axis. This makes the graph clearly demonstrate the inability of the network to separate its outputs from differences on the AP axis. It is a distortion, and if there are a large number of alphas mixed in with the higher-AP neutrons, they will be classified incorrectly.
2018_07_16_waveform_with_new_cuts: In the validation data for this test, the neural network is almost completely useless. The neutrons and alphas are distributed far and wide through the output axis, and one could not make any kind of meaningful prediction based on it. The AP is far, far more accurate. This strongly indicates two important results. One, that all information necessary for significant overfitting has been removed. There was no noise or other bias that the network could use. Two, that without the position corrections, it is impossible to distinguish between neutrons and alphas, at least using the raw waveform. This is not surprising, but there was no existing evidence that this was the case. It is known to be possible to distinguish using the volume, and this indicates that this is absolutely necessary.
2018_07_16_waveform_with_new_cuts_and_position_input: There is little of note to be seen in this experiment. Its performance is poor and tremendously unstable, with its predictions changing drastically between epochs. However, it is interesting that it is not better than the previous experiment. All information encapsulated in the 8-band frequency domain data is in the inputs to this network, but it is not learning how to use it. As such, this contradicts the evidence that the previous experiment failed because the inputs were not sufficient. Perhaps the raw waveform is not an ideal format, and it must be further processed (such as through a Fourier transform).
2018_07_17_banded_frequency_normalized: The 89% accuracy statistic is somewhat misleading. From the graph of the network's decision boundary, it is clearly far from confident. Its outputs form a vague correlation at most with the actual status of the examples. Clearly, the removed information was very important.
2018_07_17_banded_frequency_not_position_corrected: Very high discrimination accuracy was observed in this test. The graph is not perfect and there are some outliers, suggesting that it was not overfitting excessively. Indeed, this appears to be an exceptional validation set, but the network is just as accurate as AP. And, of course, it is significant that this was achieved without any kind of position correction or even position input.
2018_07_17_banded_frequency_not_position_corrected_retest: Performance here was seemingly no worse than AP. The neural network got just as many right, except that its distribution was less one-sided than AP. Where AP classified several neutrons in alpha territory, the network struck more of a balance in a way that could not be achieved through a simple transformation on the AP function.
2018_07_17_banded_frequency_not_position_corrected_with_position_input: Performance in this epoch was meaningfully better than AP, especially at the peaks of validation performance. It is also better than the performance of a network trained on the banded frequency domain data without the position of the bubble. It managed to correctly classify several examples on which AP failed. This is a strong indication that the network is gaining information from the positional data.
2018_07_17_banded_frequency_custom_fourier_transform: The networks trained here are essentially useless as discriminators. This is not unexpected, as the frequency bands used are almost certainly incorrect.
2018_07_18_banded_frequency_no_dropout: The graphs for this test showed a standard deviation for alpha particles significantly higher than AP, but lower for neutrons. Subjectively, it was not entirely accurate at picking neutrons out of the more mixed regions of the AP axis. This may be a result of overfitting on the training set.
2018_07_18_banded_frequency_25%_dropout: The standard deviations for both classes are better for the network's outputs than AP. Also, it is subjectively very accurate and reasonable in its decisions, with a fairly small number of outliers. This is the best result seen so far with the banded frequency network.
2018_07_18_iterative_cluster_nucleation_banded: The graph results for this technique are not bad per se, but they are very scattered and imprecise. There is a large amount of overlap between the neutrons and alphas. It seems somewhat reminiscent of excessive regularization. That is a difficult balance to strike for this task, since lower regularization will result in more incorrect examples being added to the ground truth set.
2018_07_18_iterative_cluster_nucleation_with_l2_0.001: On this training run, the network was never as decisive as when it was trained without iterative cluster nucleation. However, it still managed to obtain higher accuracy than AP, as well as a lower standard deviation for both classes.
2018_07_18_iterative_cluster_nucleation_increasing_threshold: Once again, there were some epochs (such as 71) that attained higher accuracy than AP as well as a lower standard deviation for both classes. This was an unremarkable change from the previous test.
2018_07_18_iterative_cluster_nucleation_with_256_initial_examples: This was very successful! While the validation set is low-noise, there are epochs when it had higher accuracy, and certainly lower standard deviations for both classes. The choices on the validation set were not as decisive as with the full training set, but were nonetheless very accurate.
2018_07_18_iterative_cluster_nucleation_without_validation_cuts: In the better-performing epochs during this run, accuracy was frequently similar to or worse than AP, even though AP is significantly less accurate on this expanded data set. That said, the standard deviation was usually similar or lower.
2018_07_18_iterative_cluster_nucleation_with_increased_regularization: Performance as measured by the graphs from this test was quite poor. The decision boundary was extremely unclear, and there was a huge amount of overlap even though AP was reasonably accurate for this data set. This provides some evidence that the low-resolution banded Fourier transform is not particularly effective for data without fiducial cuts, possibly due to the very different acoustics.
2018_07_18_iterative_cluster_nucleation_removed_second_last_layer: Performance was very poor, once again. The neutrons were sometimes reasonably clustered, but the alphas were wildly scattered across the graph.
2018_07_18_banded_frequency_no_validation_cuts_at_all: Despite the poor performance, it is in fact better than AP on the validation set. This is true, for certain epochs, in accuracy as well as standard deviation for both classes. It is by no means precise; there are outliers scattered across the graph. But this is true even more so for AP than for the neural network's output.
2018_07_19_iterative_cluster_nucleation_with_128_initial_examples: AP is very accurate indeed for this validation set. For some reason, the neural network performs poorly and seems to overfit extensively. The neutrons which are clustered together on the AP axis are classified reasonably accurately, but the alphas are scattered all over to the map, to the point where the network's outputs are almost meaningless. This may be a result of extensive overfitting.
2018_07_19_gravitational_first_test: For this run, two different data sets are saved per epoch. One that shows conventional validation accuracy, and one that compares the dynamically computed gravitational ground truths to the original, correct ground truths for the training set. The validation graphs showed rather generic poor performance, with nothing particularly notable. The training ground truth graphs showed an essentially random, meaningless split between the examples that happened to end up on the top of the graph versus those that happened to end up on the bottom, where the examples would, through overfitting, stay wherever they were initially, for the most part. This indicates that the original, correct training data does not have nearly enough influence over the original training process.
2018_07_19_gravitational_offset: It appears that the relatively small influence of the gravity means that it disturbs the training process somewhat but does not result in meaningfully confident classifications. Throughout the training run, neutrons were often reasonably clustered in the bottom left (not neutron AP outliers, though), while alphas were scattered all over the graph on both axes.
2018_07_19_banded_frequency_signal_time_bin_only_and_no_regularization: Removal of the unnecessary data and all regularization did not prevent the neural network from matching the performance of AP. It produced equivalent accuracy (with a closer to equal distribution of outliers) and a lower standard deviation for both classes.
2018_07_20_custom_fourier_transform: Surprisingly, training was actually quite successful! While accuracy was not quite as accurate as AP (and this could be a quirk of the validation set) the standard deviation was lower for both classes. This demonstrates that nothing too important was destroyed when making the custom Fourier transform (although it is only 5 bands rather than 8).
2018_07_20_divide_by_squared_number_of_samples: Once again, while accuracy is not bad, it is worse than AP. (The division seems to have made very little difference.) That indicates that perhaps the number of bands does make a difference. At the moment, I do not have access to the exact bands used, but I should be able to increase the resolution within the same overall range, and performance may increase.
2018_07_20_more_bands: The validation set happened to be very clean for this run, so its high accuracy is not very meaningful. It is no better than AP. A re-test is required.
2018_07_20_more_bands_retest: The validation set was indeed less clean. While the network attained lower standard deviations compared to AP, its accuracy was the same at the very best.
