2018_06_26_0_meaningless_predictions.json: I tried to use the banded Fourier transform to distinguish between the background runs and the calibration runs. I found it to perform poorly, and also that the AP as it is is useless. Some further cuts are almost certainly required.
2018_06_26_1_perfect_ap_simulation.json: I added a bad attempt at an AP cut. The network just ended up simulating the AP function (quite well, in fact) but since the AP was determined in the last experiment to be useless at the moment, there is nothing meaningful here.
2018_06_29_improved_bubble_number_cut.json: There seems to be light at the end of the tunnel. I replaced the image-based bubble number filter with one based on the pressure transducer. After this cut, the data, while still quite noisy, has a much clearer separation between neutrons and alphas in both the neural network and AP axes.
2018_06_29_first_proper_fiducial_cuts.json: I added the original fiducial cuts used in previous research. It is starting to look much more reasonable; there are many more neutrons than alphas, but they are distinguished into a blob on the AP axis, outliers besides. However, the neural network's outputs are almost all less than 0.5 and some even go negative.
2018_06_29_improved_fiducial_cuts_with_sigmoid.json: I added a sigmoid activation on the final layer to prevent the negative results. It now looks pretty near the graph that was presented in the paper! The alphas are vastly outnumbered by neutrons. There are still neutron outliers, and over half of them are classified as alphas. That said, almost all of the alphas are classified as alphas. Not a perfect result, but it closely resembles what is shown in the PICO-60 paper.
2018_06_30_audio_classification_with_saved_validation_set.json: This was a major failure. The apparently high accuracy turned out to be the result of the network precisely outputting the average of all of the ground truths. Every one of the examples was classified with the same output, around 0.826. This brought to my attention a key problem with the network architecture: there are not enough convolutional layers. The first dense layer has well over five million parameters, which is vastly more than all of the other layers. It is bound to not learn properly on a comparatively small data set.
2018_06_30_audio_classification_fully_convolutional_architecture.json: This was a striking success. Using a fully convolutional architecture with no dense layers up to the very end, it was able to distinguish between neutrons and alphas with ~92% accuracy on the validation set. This was not as high as the performance got; it reached ~96% in the log but the validation data for that epoch was not saved. Most significantly, it reached that degree of accuracy with very little apparent correlation to the AP score. There were many decisive alphas and many decisive neutrons, and a significant number of outliers with a totally different pattern to that seen in the banded Fourier transform data. They were spread across the AP axis, and may potentially represent particles which are incorrectly classified in the (admittedly rough and imperfect) ground truth data set.
2018_06_30_audio_classification_with_saved_models: These validation sets, saved at each epoch, showed a validation performance which well exceeded that represented in the original PICO-60 paper. The neutrons and alpha particles were very clearly separated, with only a very small number of outliers in between. It is particularly notable in this data the degree of independence of the AP axis. There was effectively no correlation between the neural network's output and the AP score, making it clear that the network was cueing on a completely different, and presently unknown, characteristic of the audio recordings.
2018_06_30_audio_classification_with_saved_models_training_set_included.json: This was a test of the same system as in the previous record, but on the training set instead of the validation set. (It is a random sample of the combined training and validation sets, so a small amount of validation data may be included.) It represented absolutely perfect separation between the neutrons and alphas except for two incorrectly-classified (or supposedly incorrectly classified, since the training data is imperfect) alpha particles. It is striking the degree to which there is a near-perfect line at 0 and a near-perfect line at 1.
2018_07_05_audio_classification_including_all_calibration_sets: This validation data showed a graph much more rough and imprecise than the previous run including only americium-beryllium data. It was reasonably accurate at placing neutrons near the bottom of the grid, but the alphas were very inaccurately destributed throughout the neural network prediction axis, to a similar degree as it was for the AP axis.
2018_07_05_audio_classification_with_fiducial_cuts: This validation data showed neutrons classified relatively close to the 0 line. A non-trivial quantity of alphas were classified accurately, but nowhere near all of them. In fact, this experiment demonstrated a near-complete disregard for the state of the alphas. It was clear the neutrons were weighted much more heavily, since there are many more of them.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy: This was a disaster. The results varied widely between predicting almost everything as 0, to scattering the neutrons and alphas more or less randomly across the spectrum. It is not clear whether this is because of the class weights, or the binary crossentropy loss function which has caused problems previously.
2018_07_05_audio_classification_with_mean_squared_error: This was almost exactly the same as the previous run. The use of a mean squared error loss function rather than binary crossentropy seemed to make almost no difference. A major difference was observed in some previous tests, but not this time.
2018_07_06_audio_classification_with_only_californium_60cm: This was very similar to previous runs including all neutron sources. It tended to cluster all examples near 1 rather than 0, but this changed significantly between epochs. It was also observed to distribute examples throughout the spectrum. It was similarly unsuccessful to previous tests.
2018_07_06_audio_classification_with_only_barium_40cm: This had, once again, almost the exact same result. Examples were distributed evenly across the spectrum. There was almost no distinction between this run, californium at 60cm, and the mix of all classes.
2018_07_06_audio_classification_americium_without_pressure_cut: Performance on this test was not nearly as good as indicated in the training logs. Neutrons were vastly outnumbered by alphas, evidently as a result of the removal of the pressure cut. The 99.2% appeared to be a result of the vast majority of alphas being near 1; the neutrons were not especially consistent. This test raised more questions than it answered: why are there so many multi-bubble events in the low background set that had to be removed? Are they alphas being misclassified as multi-neutron events, or are they neutrons? If they are, there must be many single-neutron events in the low background data set as well.
