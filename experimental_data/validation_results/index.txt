2018_06_26_0_meaningless_predictions.json: I tried to use the banded Fourier transform to distinguish between the background runs and the calibration runs. I found it to perform poorly, and also that the AP as it is is useless. Some further cuts are almost certainly required.
2018_06_26_1_perfect_ap_simulation.json: I added a bad attempt at an AP cut. The network just ended up simulating the AP function (quite well, in fact) but since the AP was determined in the last experiment to be useless at the moment, there is nothing meaningful here.
2018_06_29_improved_bubble_number_cut.json: There seems to be light at the end of the tunnel. I replaced the image-based bubble number filter with one based on the pressure transducer. After this cut, the data, while still quite noisy, has a much clearer separation between neutrons and alphas in both the neural network and AP axes.
2018_06_29_first_proper_fiducial_cuts.json: I added the original fiducial cuts used in previous research. It is starting to look much more reasonable; there are many more neutrons than alphas, but they are distinguished into a blob on the AP axis, outliers besides. However, the neural network's outputs are almost all less than 0.5 and some even go negative.
2018_06_29_improved_fiducial_cuts_with_sigmoid.json: I added a sigmoid activation on the final layer to prevent the negative results. It now looks pretty near the graph that was presented in the paper! The alphas are vastly outnumbered by neutrons. There are still neutron outliers, and over half of them are classified as alphas. That said, almost all of the alphas are classified as alphas. Not a perfect result, but it closely resembles what is shown in the PICO-60 paper.
2018_06_30_audio_classification_with_saved_validation_set.json: This was a major failure. The apparently high accuracy turned out to be the result of the network precisely outputting the average of all of the ground truths. Every one of the examples was classified with the same output, around 0.826. This brought to my attention a key problem with the network architecture: there are not enough convolutional layers. The first dense layer has well over five million parameters, which is vastly more than all of the other layers. It is bound to not learn properly on a comparatively small data set.
2018_06_30_audio_classification_fully_convolutional_architecture.json: This was a striking success. Using a fully convolutional architecture with no dense layers up to the very end, it was able to distinguish between neutrons and alphas with ~92% accuracy on the validation set. This was not as high as the performance got; it reached ~96% in the log but the validation data for that epoch was not saved. Most significantly, it reached that degree of accuracy with very little apparent correlation to the AP score. There were many decisive alphas and many decisive neutrons, and a significant number of outliers with a totally different pattern to that seen in the banded Fourier transform data. They were spread across the AP axis, and may potentially represent particles which are incorrectly classified in the (admittedly rough and imperfect) ground truth data set.
