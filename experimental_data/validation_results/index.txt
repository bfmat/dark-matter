2018_06_26_0_meaningless_predictions.json: I tried to use the banded Fourier transform to distinguish between the background runs and the calibration runs. I found it to perform poorly, and also that the AP as it is is useless. Some further cuts are almost certainly required.
2018_06_26_1_perfect_ap_simulation.json: I added a bad attempt at an AP cut. The network just ended up simulating the AP function (quite well, in fact) but since the AP was determined in the last experiment to be useless at the moment, there is nothing meaningful here.
2018_06_29_improved_bubble_number_cut.json: There seems to be light at the end of the tunnel. I replaced the image-based bubble number filter with one based on the pressure transducer. After this cut, the data, while still quite noisy, has a much clearer separation between neutrons and alphas in both the neural network and AP axes.
2018_06_29_first_proper_fiducial_cuts.json: I added the original fiducial cuts used in previous research. It is starting to look much more reasonable; there are many more neutrons than alphas, but they are distinguished into a blob on the AP axis, outliers besides. However, the neural network's outputs are almost all less than 0.5 and some even go negative.
2018_06_29_improved_fiducial_cuts_with_sigmoid.json: I added a sigmoid activation on the final layer to prevent the negative results. It now looks pretty near the graph that was presented in the paper! The alphas are vastly outnumbered by neutrons. There are still neutron outliers, and over half of them are classified as alphas. That said, almost all of the alphas are classified as alphas. Not a perfect result, but it closely resembles what is shown in the PICO-60 paper.
2018_06_30_audio_classification_with_saved_validation_set.json: This was a major failure. The apparently high accuracy turned out to be the result of the network precisely outputting the average of all of the ground truths. Every one of the examples was classified with the same output, around 0.826. This brought to my attention a key problem with the network architecture: there are not enough convolutional layers. The first dense layer has well over five million parameters, which is vastly more than all of the other layers. It is bound to not learn properly on a comparatively small data set.
2018_06_30_audio_classification_fully_convolutional_architecture.json: This was a striking success. Using a fully convolutional architecture with no dense layers up to the very end, it was able to distinguish between neutrons and alphas with ~92% accuracy on the validation set. This was not as high as the performance got; it reached ~96% in the log but the validation data for that epoch was not saved. Most significantly, it reached that degree of accuracy with very little apparent correlation to the AP score. There were many decisive alphas and many decisive neutrons, and a significant number of outliers with a totally different pattern to that seen in the banded Fourier transform data. They were spread across the AP axis, and may potentially represent particles which are incorrectly classified in the (admittedly rough and imperfect) ground truth data set.
2018_06_30_audio_classification_with_saved_models: These validation sets, saved at each epoch, showed a validation performance which well exceeded that represented in the original PICO-60 paper. The neutrons and alpha particles were very clearly separated, with only a very small number of outliers in between. It is particularly notable in this data the degree of independence of the AP axis. There was effectively no correlation between the neural network's output and the AP score, making it clear that the network was cueing on a completely different, and presently unknown, characteristic of the audio recordings.
2018_06_30_audio_classification_with_saved_models_training_set_included.json: This was a test of the same system as in the previous record, but on the training set instead of the validation set. (It is a random sample of the combined training and validation sets, so a small amount of validation data may be included.) It represented absolutely perfect separation between the neutrons and alphas except for two incorrectly-classified (or supposedly incorrectly classified, since the training data is imperfect) alpha particles. It is striking the degree to which there is a near-perfect line at 0 and a near-perfect line at 1.
2018_07_05_audio_classification_including_all_calibration_sets: This validation data showed a graph much more rough and imprecise than the previous run including only americium-beryllium data. It was reasonably accurate at placing neutrons near the bottom of the grid, but the alphas were very inaccurately destributed throughout the neural network prediction axis, to a similar degree as it was for the AP axis.
2018_07_05_audio_classification_with_fiducial_cuts: This validation data showed neutrons classified relatively close to the 0 line. A non-trivial quantity of alphas were classified accurately, but nowhere near all of them. In fact, this experiment demonstrated a near-complete disregard for the state of the alphas. It was clear the neutrons were weighted much more heavily, since there are many more of them.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy: This was a disaster. The results varied widely between predicting almost everything as 0, to scattering the neutrons and alphas more or less randomly across the spectrum. It is not clear whether this is because of the class weights, or the binary crossentropy loss function which has caused problems previously.
2018_07_05_audio_classification_with_mean_squared_error: This was almost exactly the same as the previous run. The use of a mean squared error loss function rather than binary crossentropy seemed to make almost no difference. A major difference was observed in some previous tests, but not this time.
2018_07_06_audio_classification_with_only_californium_60cm: This was very similar to previous runs including all neutron sources. It tended to cluster all examples near 1 rather than 0, but this changed significantly between epochs. It was also observed to distribute examples throughout the spectrum. It was similarly unsuccessful to previous tests.
2018_07_06_audio_classification_with_only_barium_40cm: This had, once again, almost the exact same result. Examples were distributed evenly across the spectrum. There was almost no distinction between this run, californium at 60cm, and the mix of all classes.
2018_07_06_audio_classification_americium_without_pressure_cut: Performance on this test was not nearly as good as indicated in the training logs. Neutrons were vastly outnumbered by alphas, evidently as a result of the removal of the pressure cut. The 99.2% appeared to be a result of the vast majority of alphas being near 1; the neutrons were not especially consistent. This test raised more questions than it answered: why are there so many multi-bubble events in the low background set that had to be removed? Are they alphas being misclassified as multi-neutron events, or are they neutrons? If they are, there must be many single-neutron events in the low background data set as well.
2018_07_06_audio_classification_all_classes_very_deep_architecture: This was a surprising and promising result. Validation accuracy was fairly poor, because for many epochs, almost all examples were predicted as being above the 0.5 line. However, particularly in epochs 17 and 18, there was a very clear division between alpha particles and neutrons; it was very accurate, in fact. The line was just at the wrong location, well above 0.5. This provides some strong evidence that it is in fact possible to accurately discriminate, even when whatever bias was in the AmBe data set has been removed.
2018_07_07_audio_classification_all_classes_very_deep_architecture_training_set_included.json: This was a test of the very deep architecture on its training data set. It demonstrated almost the exact same line of separation (~0.9) as the validation data, demonstrating that this clear distinction is not a fluke. The trained network for epoch 18 (the second-last epoch) was used for evaluation.
2018_07_07_audio_classification_all_classes_very_deep_with_dense: The modified architecture including a dense layer shows promise. Most epochs demonstrated poor performance reminiscent of overfitting, with the exception of epoch 15, which proved a very effective discriminator. Its standard deviation is significantly better than that of AP on the data without fiducial cuts. Most bubbles cluster near 0 and 1, with a small number of outliers (which may plausibly represent incorrect ground truths).
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01: It is very evident from these validation sets that the regularization lambda parameter of 0.01 is very excessive. The behavior changes drastically between epochs, but it ranges from a meaningless cloud of points to a collection of all points at a specific average line. All models would be essentially useless as a discriminator.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001: The regularization lambda seems to still be a bit too high. There were some adequately accurate discriminators present within the various epochs, but none of them were decisive. They all resulted in a fairly wide cloud of outputs with a large standard deviation. Despite this, the best model managed ~91% accuracy.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003: This was the best model seen so far. It seemed to be a nearly optimal balance of the decisiveness of an unregularized model and the skepticism of a regularized model. Except for a single-digit number of outliers, bubbles were divided into defined lines at 0 and 1. Importantly, these lines were not composed only of the ground truth class belonging to that line. Some suspected alphas were classified decisively as neutrons and vice versa, indicating that the moderate degree of regularization gave the model freedom to ignore these examples rather than overfit to them. Once again, it was still sporadic and there were many bad epochs, but the good epochs (particularly 15) demonstrated apparently nearly ideal performance for this data set.
2018_07_09_audio_classification_with_zero_image_bubble_events: Performance here was comparable to previous tests but not ideal. Epoch 17 (with ~93% validation accuracy) had a large number of outliers not definitively classified into either of the two classes, although most of these were on the right side.. Epoch 15 (with ~91% accuracy) was more decisive, if apparently less accurate. Once again, there were epochs with completely useless performance, including at least one (epoch 19) which confidently classified almost every example as a neutron.
2018_07_09_audio_classification_with_cropped_inputs: Performance was very poor, even on the first epochs. The 81% accuracy statistic amounted to a rough cluster of neutrons at the bottom, a few sparse alphas at the top, and a large number of seemingly randomly placed bubbles across the graph. This effectively confirms the fear that there is some kind of bias in the noise at the beginning or the hydraulic sounds at the end of the recording, which can be used to discriminate.
2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization: This was certainly an improvement on the previous test, if not ideal. In the graph for epoch 16, the alphas were reasonably pure. Almost all of the neutrons were classified correctly. There was a significant number of alpha outliers confidently placed in neutron territory, enough that the standard deviation for that set of bubbles was higher than that for the AP discriminator.
2018_07_09_audio_classification_with_residual_blocks_on_every_convolutional_layer: As indicated in the training log, this run was remarkably stable. Performance was not good, and the graphs demonstrated very poor discrimination, but the network was consistently decisive (there were no aimless clouds) and its decisions remained almost the same on various epochs throughout the training run.
2018_07_10_audio_classification_with_more_layers_and_residual_blocks_on_every_two: The graphs for this test were surprising. The network showed a very extreme degree of decisiveness. There was a line at 0 and a line at 1, with no lack of confidence. Unfortunately, these lines were not accurate classifications. In epoch 10, a significant mass of neutrons (interestingly, clustered together in AP as well as network outputs) were confidently classified as alphas. However, the discrimination for epoch 3 was reasonably accurate; not perfect, but not terribly far off. However, it is questionable whether or not this degree of confidence is desirable, when so many are classified incorrectly.
2018_07_10_audio_classification_with_more_layers_and_no_residual_blocks: This was very similar to the previous run. It was very confident but frequently incorrect. The 73% accuracy peak is completely unacceptable for any kind of meaningful discrimination.
2018_07_10_audio_classification_validation_with_fiducial_cuts: This was a much more interesting and successful test. Reversion to the old network architecture improved things substantially. Validation performance on the set with fiducial cuts applied was very consistent; the graphs demonstrated reasonably accurate classification on the neutrons. The alphas were more questionable; many of them were on the wrong side. It is possible biases in the full data set are disturbing performance on the subset that passes fiducial cuts.
2018_07_10_audio_classification_trained_with_fiducial_cuts: After the recent demonstrations of strong stability in training and validation accuracy, this was a return to the rapid oscillations and inconsistent performance observed previously. It may be because of the smaller amount of training data, and the network finding new and creative ways to overfit on the dataset with each passing epoch. One epoch showed all of the bubbles clustered at 0, and one showed the neutrons mostly classified correctly but the alphas mostly disregarded.
2018_07_10_initial_high_resolution_frequency_domain: The graphs of this experiment are very simplistic and contain no meaningful information. Frequently everything is classified as a perfect 0 (neutron), or everything except for a small number of apparently memorized examples (which are sometimes incorrect themselves).
