2018_06_27_first_training_run.out: The very first time I trained a neural network on the supercomputer. It was a simple multi-layer neural network trained on the banded Fourier transformed data without AP cuts. It was just a proof of concept.
2018_06_28_first_image_classification.out: A convolutional neural network was trained to distinguish between images from the background run and the americium-beryllium calibration run. Validation data did not work. An unexpected success at ~96% accuracy on the training set; however, I was wary of overfitting because of the very large number of parameters in the first dense layer.
2018_06_28_second_image_classification.out: Very similar to the previous experiment. However, the proportion of bubbles randomly omitted was reduced from 0.8 to 0.5. Also, the number of images held in memory was increased from 256 to 512, and the number of images turned over per batch was increased from 2 to 16, resulting in a greater exposure to different training examples. I added another convolutional layer to reduce the number of parameters in the network. Despite this, it managed to obtain ~97% accuracy on the training set.
2018_06_29_first_audio_classification.out: A one-dimensional convolutional neural network was trained on the raw time domain audio waveforms to distinguish between americium-beryllium calibration runs and background radiation runs. Dropout regularization was included between all layers to alleviate overfitting. It was slow to achieve good performance, but managed 97% accuracy at peak and 93% at the end. This was, once again, unexpectedly high. Validation did not work, as in previous tests with the generator training system.
2018_06_29_first_proper_fiducial_cuts.out: Performance is apparently high, but it seems to be predicting neutron for almost every example. The alphas are so vastly outnumbered that they aren't contributing significantly. The maximum prediction was less than 0.6, so this result is unsurprising.
2018_06_29_improved_fiducial_cuts_with_sigmoid.out: This was a very significant improvement in a way that is not clear in the training performance. Rather than predicting everything to be a neutron, the network is overzealous in classifying as alphas. Many neutron outliers in the louder range of the AP spectrum are errors as such. Almost no alphas are incorrectly classified as neutrons, and most of the ones that are, are extreme outliers. This is, once again, strikingly similar to the graph in the PICO-60 paper.
2018_06_29_image_classification_with_validation.out: Validation set is apparently functional, but its behavior is very strange. It starts off with ~80% accuracy and retains that all the way through the run. Also, the performance in general is much worse than that of the previous image classification run.
2018_06_29_image_classification_with_validation_and_fiducial_cuts.out: This time, the validation score meaningfully changes throughout the training run. However, it is very bad, peaking at around 60%. Training scores are also bad, but significantly better than the validation score.
2018_06_29_image_classification_with_validation_and_height_cuts_only.out: This was done without the standard fiducial cuts but with a "hack" cut that was included in the earlier training runs. It is not entirely clear why this cut made such a large difference, but it did. Rather than not being able to fit at all, it fit with 99.7% accuracy, but only 51% on the validation set (which is worse than random due to the asymmetric class distribution). This clearly answered the question as to whether or not overfitting is the cause for this high accuracy with a resounding yes.
2018_06_30_audio_classification_with_validation.out: This did not include the fiducial cuts at all, even omitting the basic height cut that made the image classification work. Performance was quite good, peaking at ~87% on the training data. Validation performance was confusing like previous tests, starting around 90% and retaining that. It seems like it was not overfitting, but this is not entirely clear based on the strange validation results.
2018_06_30_audio_classification_with_saved_validation_set.out: This peaked at roughly the same performance as the previous run, and had the same parameters except that the validation set was saved at the end. It revealed the cause of the equal validation performance: the network ended up outputting the average of the ground truth values for all of the validation examples. It is not clear why the training performance was lower (below 30%) at the beginning; it may have had to do with the dropout used during training but not validation.
2018_06_30_audio_classification_fully_convolutional_architecture.out: The validation performance oscillated strangely up and down during this run. It peaked at ~96%, but went back down to ~42% even as the training loss continued to decrease and the training performance continued to increase. The last epoch was fairly successful, but not the most in the run. Also, the 99%-100% accuracy in the training dataset was very high relative to previous comparisons. This experiment was done without any fiducial cuts.
2018_06_30_audio_classification_with_saved_models.out: This was an even more notable success. The accuracy remained at 100% for some period of time, and the loss function was extremely low. When the training data was reprented in a graph, it demonstrated almost perfect separation between neutrons and alphas. The validation set was slightly noisier, but not very much; it too was very precise and independent from the AP axis.
2018_07_05_audio_classification_including_all_calibration_sets.out: This was done with all of the neutron calibration data sets combined. It compared unfavorably with previous experiments; training accuracy was fairly high but validation accuracy was between 70% and 80%. It was evidently overfitting much more than was observed with only the americium-beryllium data set.
2018_07_05_audio_classification_with_fiducial_cuts.out: This was done with all of the neutron calibration sets, and with the fiducial cuts enabled. It didn't overfit quite as much as the previous experiment, but it was also not successful. Almost perfect accuracy was seen on the training data, but validation peaked around 90%. The graph proved this relatively high number to be almost meaningless.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy.out: This was the same as the previous test, except that class weights were added so the alphas were weighted 8 times as heavily as the neutrons, and the loss function was changed from mean squared error to binary crossentropy. The loss function was not good; validation varied widely from 80%-90% down to 40%, sometimes between just one epoch. Training was very accurate, at ~99%.
2018_07_05_audio_classification_with_mean_squared_error.out: This was same as the previous run, except that the binary crossentropy loss function was changed back to mean squared error. The class weights were left in place. Performance was very similar, demonstrating high training accuracy but poor validation accuracy (~85%). Overfitting was to a very similar degree.
2018_07_06_audio_classification_with_only_californium_60cm.out: The network architecture was reverted back to the same as the one that trained near-perfectly on the americium-beryllium calibration set. Instead, it was trained to distinguish between low background data and californium at 60cm. Performance was, unfortunately, very similar to the tests with mixed datasets. Training accuracy was high (~99%), but validation accuracy was very low (sub-70%).
2018_07_06_audio_classification_with_only_barium_40cm.out: This was the same as the previous run, with californium at 60cm switched out for barium at 40cm. It had a very similar training result: training performance was very high as usual, but validation peaked at ~64%. This makes it increasingly clear that the americium data set is unusual in that there is a clear giveaway that separates it from the calibration run without any corrections, given that neither californium at 60cm nor barium at 40cm demonstrated this property.
2018_07_06_audio_classification_americium_without_pressure_cut.out: This was a test of the original system used on the americium data, trained on the americium data. The difference was that the pressure cuts to determine the number of bubbles were removed. The resulting accuracy was very strange. Training accuracy was very high, as expected. Validation accuracy reached ~98% in the earlier epochs, but then dropped to ~35%. For the last epoch, it achieved 99.2%, after remaining at a very low accuracy for several epochs, including the one immediately before. This is notably higher than the best validation accuracy achieved in previous americium tests. It is not at all clear what changed in the training process to make it jump up so sporadically.
2018_07_06_audio_classification_all_classes_very_deep_architecture.out: This was a test of all of the classes without weighting, using a very deep architecture based on M34 (which was designed from the ground up for processing of raw audio waveforms). It did not include even a single dense layer; the final output was the result of a global average pooling layer. It appeared not to make a great difference to validation performance; it jumped between ~71% and ~31%. Training accuracy was, as always, above 99%. However, the results of the last test brought into question the integrity of the data set used to train this as well as previous models.
2018_07_07_audio_classification_all_classes_very_deep_with_dense.out: The architecture used for this test was something of a balance between the original model with relatively few layers and the extreme M34-inspired architecture. The size of the max pooling layers was increased from 6 to 8 (to reduce the number of outputs on the final convolutional layer) and the average pooling was replaced with a single dense layer. Training performance was actually more limited (around 97%). Validation performance oscillated quite significantly, peaking around 91%.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01.out: The regularization used here in an attempt to alleviate the overfitting observed in the previous test seems to have been overkill; the training accuracy was low (less than 92% at all times) and so was the validation accuracy. In addition, the validation accuracy still oscillated widely like during previous experiments.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001.out: The performance for this run was very similar to what it was without regularization, except that the training data performance was somewhat lower (~94% rather than ~97%). It had a very similar maximum performance, as well; around 90%. It reached around this performance on a few different epochs rather than just one. However, it is still very unstable and changes a lot between epochs.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003.out: This run performed arguably better than any of the previous ones; its peak accuracy was ~91%, and it managed to maintain approximately this for more than one epoch, as well as reaching that peak twice. It was still sporadic, and no model tested so far solved this problem. Also, it is beginning to look like there is a practical limit of ~91% accuracy on validation using this data set. This appears to be a reasonably moderate value for the regularization lambda parameter.
2018_07_08_first_iterative_cluster_nucleation.out: It is not clear what the problem is in this instance. On the first epoch, the network achieves a training accuracy of 95%. After that, as it adds more data to the training set, its performance degrades and hovers around ~50% for the rest of the experimental run. It is possible that the high-confidence examples are essentially random after such a short training period, and more time (and potentially regularization) is required for a meaningful result.
2018_07_09_audio_classification_with_zero_image_bubble_events.out: This run did not change any network or training parameters versus previous audio classification trials. A problem with the multi-bubble data cut was fixed; there were many events with 0 bubbles recognized by the camera which were cut since there was apparently not 1 bubble. It was changed to include events with 0 or 1 bubbles, provided the pressure transducer cut also accepted them as having approximately 1 bubble. As always, training was sporadic, but the peak of validation performance here was higher than ever before, at ~93%.
2018_07_09_audio_classification_with_normalized_recordings.out: This changes nothing from the previous run, except that the recordings are normalized to have a geometric mean (standard deviation off of 0) equal to 1. Based on the accuracy statistics, this seems to not have had any ill effect. None were expected, but this was important to confirm before adding any kind of noise or other transformations.
2018_07_09_audio_classification_with_cropped_inputs.out: For this run, the audio recordings were cropped, removing the first 90000 samples (which are noise at the beginning) and the last 60000 (which are very loud recordings of the hydraulic system re-pressurizing the vessel after a bubble forms). There was evidently some information in the data that has been removed that could be used to discriminate. However, it is not data that should be provided, and any accuracy that relies on this data amounts to overfitting. Performance in this run was considerably worse; training accuracy peaked at ~88%, and validation at ~81%. It is possible the regularization used to prevent overfitting when accuracy was high is hindering performance. Interestingly, validation accuracy was significantly less sporadic in this run. It still oscillated, and in fact got quite significantly worse at one point during the middle of the test, but these were slower, more gradual changes as compared to the rapid and extreme oscillations observed in previous tests.
2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization: This run had regularization disabled, to see if it alleviated the lack of fitting observed in the previous test. It seems it may have improved the situation somewhat, with the validation accuracy peaking at ~86% (and this exact accuracy maintained for two epochs, 16 and 17). Notably, it does not seem to be overfitting appreciably; the training accuracy corresponding to that peak was ~88%.
2018_07_09_iterative_cluster_nucleation_multiple_epochs_per_iteration.out: This was a test of a mostly identical iterative cluster nucleation system. Choices for further training data were apparently poor in the first test, so multiple epochs were run before selection of further training data during this test. Unfortunately, it did not appreciably improve the section. Training accuracy during the first iteration settled at ~95%, but this figure dropped very far during the second and later iterations (always below 70%). The cause of this is unclear at the moment; there may be some issues with the training data selection system.
2018_07_09_audio_classification_with_residual_blocks_on_every_convolutional_layer.out: The model for this test had a residual block around every convolutional layer, meaning that the input and the output of the layer are added together. This is not entirely faithful to the original M34-res architecture, but was an attempted proof of concept. However, it was a failure. Validation was very stable, mostly remaining between 70% and 80% with slow, gradual oscillations. There were no peaks outside of this range. Training accuracy was higher, but not very much; it never reached even 85%. It is possible the residual blocks around every layer are detrimental, and they should instead be around every two layers as in the original paper.
2018_07_10_audio_classification_with_more_layers_and_residual_blocks_on_every_two.out: This was a disaster. The network had almost twice as many convolutional layers, and residual blocks on every two layers, as in the M34-res architecture. Training accuracy never reached even 75%, and validation accuracy peaked at 78% very early in the run. Several epochs had a validation accuracy below 40%. It was also not stable like the previous test; there were very significant oscillations.
2018_07_10_audio_classification_with_more_layers_and_no_residual_blocks.out: The poor performance on both runs with residual blocks raised questions about whether or not they are effective at all for this task. This test used the same architecture as the last run (with almost twice as many layers as previous runs) but without any residual blocks. It was yet another mediocre performance. Training accuracy hovered around 70%-75%, and validation accuracy peaked at ~73%. It is noteworthy that this performed worse than a model with around half as many layers and no particular features that should give it an edge (2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization).
2018_07_10_audio_classification_validation_with_fiducial_cuts.out: This was the same as the recent relatively successful run, except that validation was judged strictly based on the data including fiducial cuts. This gives a more direct comparison against the accuracy of AP for discrimination. Training accuracy was consistently high at ~96% for the last few epochs, which is curiously higher than the last time this system was tested. Validation performance was fairly stable around 90% towards the ending, which is notably better than was obtained with the original full data set. There is clearly some degree of overfitting; it is possible re-introducing regularization is necessary.
2018_07_10_audio_classification_trained_with_fiducial_cuts.out: This was an attempt to determine whether or not training on the rest of the data improved performance on the data after fiducial cuts. There was reason to be skeptical of the benefits of training on the wall events, because the acoustics were said to be so different. Surprisingly, the training accuracy was worse (~93% rather than ~96%). Validation performance was similar (~91%) but less consistent, and had a lower consistent accuracy. At the moment, there seem to be no benefits to training only on data after fiducial cuts.
2018_07_10_initial_high_resolution_frequency_domain.out: This initial test of the frequency domain training system was a failure for reasons that are not entirely clear. A Fourier transform of the audio data, divided into 16 time bands and 1024 frequency bands, was flattened and fed into a dense neural network. The validation accuracy stabilized around 68% before diving to ~32% at the very end, and the training accuracy was notably worse (around 50% for much of the run).
2018_07_10_audio_classification_with_secondary_position_input.out: This was a test of a modified convolutional network architecture, where the position of the bubble was concatenated with the outputs of the last convolutional layer, to be input into the first dense layer. The objective was to simulate the positional corrections used to compute the AP, within the neural network. Normalization was disabled so that magnitude (combined with position corrections) could be hopefully taken into account. Training accuracy was reasonably high (~97%) but validation accuracy was consistently below 70%. This seems like a textbook case of overfitting.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.0003.out: This introduced L2 regularization to the same setup as the previous test. Surprisingly, its validation accuracy was still much worse than before the additional input was introduced, but better than before any regularization was introduced. It is apparently still overfitting on the new inputs. Training accuracy is somewhat better than before the new input was introduced (~94%, up from ~88%), but this represents nothing other than overfitting.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.001.out: The increased regularization did nothing to alleviate the overfitting. Validation accuracy peaked at ~75%, and training accuracy was very similar to what it was before. Its accuracy was very stable, but not good, oscillating generally between 65% and 75%. For experiments in the immediate future, the regularization lambda parameter will be decreased back to 0.0003 because of how the larger parameter caused underfitting problems the last time it was tested.
2018_07_12_audio_classification_with_l2_0.0003_and_no_barium_data.out: For this experiment, the regularization lambda was reverted to its original value. Also, the barium data sets were removed from the neutron class. They are gamma calibration sets, not neutron, so they are invalid for the context of discriminating between alphas and neutrons. The result was slightly better but not nearly enough. Validation accuracy peaked at ~80%, and was not overly stable (it oscillated around the range of ~65% to ~80%). Training accuracy demonstrated overfitting; at the peak validation epoch, it was ~92%, and it went up from there later on.
2018_07_15_banded_frequency_new_validation_cuts.out: This is the same network used during the very early tests with the banded frequency domain data. Surprisingly, its performance is quite poor. Training accuracy peaks at ~89%, and validation at ~88%. Particularly for validation, this number is unstable, where it frequently dipped below 80%, even near the end of training.
2018_07_15_banded_frequency_smaller_architecture.out: This was a slight improvement, with one layer removed and the other reduced to 12 neurons. Validation accuracy peaked at ~89%, and did not overfit substantially.
2018_07_16_banded_frequency_validation_not_removed.out: There was a significant bug where, rather than just the validation data being removed from the training set as was intended, all data which passed the validation tests (most of which were supposed to be used for training) were removed from the training data. This made a very significant difference to the result: validation accuracy peaked at 96%, which was significantly higher than the ~88% training accuracy present for much of the training run.
2018_07_16_banded_frequency_training_data_passes_validation_cuts.out: This apparently now does not have any disadvantages as compared to AP. Its accuracy for binary classification is slightly higher, and its standard deviation for both classes is lower. Interestingly, the validation accuracy is slightly lower after training on only data that passes the validation tests (although this could just be a case of a slightly more difficult randomly selected validation set).
2018_07_16_banded_frequency_linear_regression.out: This is still trained only on data that passes the validation cuts. The linear regression model has a lower validation accuracy than the previous tests (~92%), although it is remarkably stable. Training accuracy stayed reliably above 90%, frequently with a noticeably higher accuracy than the validation data. This is interesting, with such a simple model.
2018_07_16_banded_frequency_validation_passing_weighted_3_times.out: This is a reversion to the neural network model. It is no longer just trained on the data that passes validation tests. It includes all data that passes the standard tests, but the examples which pass the validation tests are weighted 3 times as heavily for training purposes. This is an improvement in absolute accuracy; it peaks at ~97% and is somewhat stable at ~96%. The training accuracy never stabilizes above 90%, so it is still clear that the realities of the training data is not unduly dragging down the accuracy on the validation data.
2018_07_16_banded_frequency_weight_reduced_to_2_times.out: The weight of the data which passes validation tests has been reduced to 2 times that of normal data. The validation accuracy is stable, but at a much lower level (93%). It seems that the greater weight has a positive impact on the network's validation accuracy. Also notably, the training accuracy seems to be minimally affected by all of these changes, remaining around 89% much of the time.
2018_07_16_waveform_with_new_cuts.out: This was a sanity test of the waveform training system. A lot was changed since it had last been experimented with. It is increasingly seeming that the overall volume of the event is a necessary parameter for discrimination, and that the normalized (or non-corrected) frequency distribution is not sufficient, once the main causes of overfitting have been removed and the validation data has been cut to the segment where AP is an accurate discriminator. The results from this training run are in line with these expectations. Training accuracy is suboptimal, remaining mostly between 70% and 80%. The validation accuracy is in a similar range. This does not signify any kind of meaningful accuracy. This network does not have a positional input, so it would be unable to perform any kind of internal corrections. Therefore, it is unexpected that high accuracy would be obtained. In a recent observation, it was determined that by listening to the audio recordings, a human can hear the difference between alpha particles and neutrons. However, it is highly dependent on the loudness of the initial sound. The neural network is likely similar.
2018_07_16_waveform_with_new_cuts_and_position_input.out: This improved nothing, surprisingly. Both the training and validation accuracies were very similar to before the position input was added. It is possible it is difficult or impractical for the network to learn to make good use of this information. It may be worth trying to incorporate positional corrections into the input audio directly. This also draws some question to the previous conclusion that it is impossible to discriminate without the overall volume, since the only other information needed to calculate the overall volume (the position) made almost no difference to the performance.
2018_07_17_banded_frequency_normalized.out: This was trained on the banded frequency domain information, normalized to a mean of 0 and a standard deviation of 1. Its performance decreased substantially, validation accuracy peaking at 89%. This is further evidence that there is meaningful information contained in the overall volume, and that an accurate decision cannot be made without it.
2018_07_17_banded_frequency_not_position_corrected.out: This was a surprising result. Banded frequency domain data without any position corrections was used, without any kind of positional input. Validation accuracy of ~98% was achieved very quickly. This was appreciably higher than the training accuracy, indicating that the validation set may have been unusually organized for this run. Still, it demonstrated greater accuracy than AP.
2018_07_17_banded_frequency_not_position_corrected_retest.out: This was a retest of the previous run. The only difference was that data from the first piezo was removed (which should change nothing, because it consisted of nothing other than -1). It presumably had a more representative validation set, since the validation accuracy only peaked at ~92%. This is worse than the non-corrected data, so it is a possibility that some important information has been removed.
2018_07_17_banded_frequency_not_position_corrected_with_position_input.out: This is clearly an improvement over the last run. It appears that the position input, which hopefully allows for some internal corrections, does make an appreciable difference. Validation peaks at ~95%. It is clearly not stable there, but it does reach it early in the training process. It is possible there is some degree of overfitting later on. Once again, validation accuracy is somewhat higher than training at times.
2018_07_17_banded_frequency_custom_fourier_transform.out: For this test, the banded Fourier transform taken from the piezo_E variable in the ROOT file was replaced with a custom Fourier transform with the same numbers of time and frequency bands. Performance was terrible; validation accuracy was around 60%. Training accuracy went slightly above 80%, but was still not very good. It seems very likely that the frequency bands used are not right. The PICO-60 paper states that they are between 1 kHz and 300 kHz, but the exact frequency ranges used are not documented.
2018_07_18_banded_frequency_no_dropout.out: This was a test to determine whether or not dropout regularization is necessary. Certainly, more overfitting was observed in this test; training accuracy is ~99%. However, it did not stop validation accuracy from going above 95%, so it is possible this is not overly detrimental.
2018_07_18_banded_frequency_25%_dropout.out: This was a middle ground between the 50% dropout used individually and no dropout at all. It still overfit somewhat, but the difference was very slight (often fractions of a percent). It also reached a peak of ~96% accuracy, which was very nearly as good as the maximum training accuracy.
2018_07_18_iterative_cluster_nucleation_banded.out: This was a full test run of iterative cluster nucleation with the banded frequency network. It failed to reach high validation accuracy, because few training examples were ever added (the maximum was 677, up from 512 initially). However, every single example that was added was correct, which is a very positive sign for this technique. The regularization may be overkill, or the threshold may be too low.
2018_07_18_iterative_cluster_nucleation_with_l2_0.001.out: This performed much better than the previous test. It never added anywhere close to the entire training set (a maximum of 836) but still managed to attain quite high validation accuracy of ~96%. This was not a fluke; it was obtained on multiple epochs and there were may more with ~95%, et cetera. The training accuracy was still higher, but this may be attributable to the, at times, exceedingly long training runs without any new data.
2018_07_18_iterative_cluster_nucleation_increasing_threshold.out: Interestingly, this did not improve validation accuracy. It peaked around ~95%, and most epochs were around ~92%. Nor did it alleviate overfitting; training accuracy was frequently at 100%.
2018_07_18_iterative_cluster_nucleation_with_256_initial_examples.out: This was a striking success. The size of the training set was tripled over the course of the training process, and it has ~99% accuracy for those new training examples. On the validation set, it reached ~98%. Admittedly, this was a low-noise validation set.
2018_07_18_iterative_cluster_nucleation_without_validation_cuts.out: Performance here was certainly worse than it was before the validation cuts. There is a very extreme degree of overfitting; training accuracy is frequently between 98% and 100%, where validation rarely passes 92%. More regularization might be required for the larger dataset.
2018_07_18_iterative_cluster_nucleation_with_increased_regularization.out: The increased L2 and dropout in this run seem to have done almost nothing to alleviate overfitting. Rather, they decrease performance on both training and validation. Validation performance was very frequently below 85%, while training was often around 97%. The expansion of the training set was reasonably accurate, but not as much as in previous tests.
2018_07_18_iterative_cluster_nucleation_removed_second_last_layer.out: For this, the changes to the regularization and threshold were reverted and the second-last layer of the network was removed. Even that made almost no difference to the result. Validation accuracy never reached even 90%, while training accuracy was between 97% and 99%.
2018_07_18_banded_frequency_no_validation_cuts_at_all.out: For this, no fiducial or audio- or pressure-based wall cuts were used at all, even for the validation set. The basic banded frequency training system (with positional input) was used, rather than iterative cluster nucleation, to minimize the number of variables. Performance is not particularly good, always remaining under 80% for validation. But, interestingly, training accuracy is not much better, always under 90%. This indicates that it is not easy at all to learn anything meaningful from the 8-band Fourier transform without wall cuts.
2018_07_19_iterative_cluster_nucleation_with_128_initial_examples.out: Performance at the beginning of this training run showed some promise, but later it started to fall apart. By the later epochs, performance was well below 90%, around ~85%. Also, many incorrect classifications were made as data was added to the training set. It is not entirely clear why this occurred when the training set was reduced to 128 examples; these incorrect classifications mostly take place later during the run, when there were well over 256 examples.
2018_07_19_gravitational_first_test.out: This was the first test of the gravitational ground truth unsupervised learning system. For this training log, training accuracy is meaningless (since the ground truths are analog) and the training loss is a better indicator of overfitting. The validation accuracy is poor (~80%) throughout much of the training run, while the training accuracy becomes very optimized. The log does not indicate whether or not the training decisions are correct as per the original ground truths.
2018_07_19_gravitational_offset.out: Previously, the output of the gravitational squashed-sigmoid function produced the ground truth alone. This does not really make sense; it means that the samples for which we have no ground truth (which outnumber those for which we do) have an outsized effect. Really, they should have fairly little impact on the training process, at least at the beginning. The gravitational function was changed to output between +0.05 and -0.05, and is added to the network's current prediction rather than replacing it. Despite this, performance is still very poor, and the network's overfitting is very evident. For the most part, the valiation accuracy remains below 80%.
2018_07_19_banded_frequency_signal_time_bin_only_and_no_regularization.out: This was a test of the regular banded frequency training system, using only the signal time bin. It is recommended to use only this time bin for analysis. Thankfully, cutting out the other data did not make performance appreciably worse. 96% on validation was still perfectly attainable. Moreover, it seemed to make regularization essentially unnecessary; overfitting was essentially nonexistent even without dropout or L2.
2018_07_20_custom_fourier_transform.out: This was the first functioning test of the custom Fourier transform system, which is currently intended to repliate the one the PICO team developed. It makes use of an approximate Python translation of the MATLAB code used by PICO to calculate piezo_E. There are many debug outputs at the beginning. Notably, the resonant energy values are many orders of magnitude larger than they are in piezo_E; this should be fixed. Despite this, the network managed to train successfully. Performance reached ~94% on the validation set, which represents that it is training very effectively.
2018_07_20_divide_by_squared_number_of_samples.out: This is a minor fix to the resonant energy calculation algorithm, which may or may not have worked. The outputs still seem to be a few orders of magnitude too high. (This may well be completely harmless; no information is lost.) Validation accuracy actually decreased compared to the previous test. However, this may represent nothing more than a fluke of the validation set or the initial weights.
2018_07_20_more_bands.out: The increase to 10 bands within the same overall frequency range seems to have had a very positive effect on accuracy. Validation peaked at ~95% and was quite stable there. It was higher than training accuracy, which remained under 90% for the most part. (This was likely due to the dropout regularization.)
2018_07_20_more_bands_retest.out: Validation accuracy was much worse for this test, peaking at ~93% (but not stable there). This is likely very similar to the previous one, with a less clean validation set.
2018_07_20_20_bands.out: This test doubled the number of bands again, to 20. Like the last run, validation accuracy peaked at ~93%, but it remained there for some time. It is difficult to extract meaning from this result before seeing the graph of the validation set.
2018_07_20_40_bands.out: The number of bands was doubled again, to 40. This did not improve accuracy; validation peaked at ~91%. This could be a sign of a messy validation set. However, unlike previous runs, the training accuracy was actually higher (~94%) so it is possible some conventional overfitting is taking place.
2018_07_20_40_bands_tanh_activation.out: For this run, a hyperbolic tangent activation was used in place of rectified linear. This changed the validation accuracy very slightly (up to ~91%). It seems to have done very little for the overfitting problem, except that training accuracy is now up to ~95% at its peak. It seems very likely that further regularization is required.
2018_07_20_l2_0.003.out: L2 regularization with a lambda of 0.003 was added in the hopes of reducing overfitting. It increased validation accuracy compared to the previous epoch, peaking at ~93% (but not stable there). Overfitting seems to have been reduced, but not entirely eliminated; training accuracy peaked at ~95%.
2018_07_20_20_bands_no_regularization.out: For this test, the Fourier transform was reverted to 20 bands, but the 50% dropout was eliminated. This change was initiated because the training accuracy was worse than the validation accuracy in some previous tests. It seems to have improved the situation; validation accuracy peaks at ~94% while training reaches ~99%. This is certainly overfitting, but it may or may not be detrimental.
2018_07_20_no_wall_cuts.out: Both validation and training accuracy were certainly lower than before. However, a peak of 92% was attained for validation accuracy, and this was without any overfitting. (Some minor overfitting was observed on other epochs, but not these peaks.) This is not optimal accuracy, but is better than has been observed before (in a test that has not been invalidated) for data without any wall cuts.
2018_07_20_no_wall_cuts_retest.out: Apparently a more representative validation set means much worse performance. Validation accuracy peaked at ~86%, and training accuracy peaked at ~92%. This is very similar to training accuracy on the previous test, which confirms the hypothesis that the validation set was unusually clean.
2018_07_21_full_resolution_fourier_transform.out: This test used the same training cuts as the previous one (meaning no wall cuts). The Fourier transform was not banded at all; it was input into the network at its full resolution. There was certainly some overfitting observed (especially since no regularization was used); training accuracy peaked close to 100%. However, validation accuracy peaked at 96%, which is much higher than the last test. This may demonstrate some non-trivial advantages for the full resolution.
2018_07_21_dropout_0.5.out: For this run, 50% dropout was used on all layers, including the audio input. It certainly reduced the training accuracy (it peaked at ~96%), but also seriously damaged the validation accuracy, which peaked at only ~91%. It may be that 50% is excessive in general, or that the network has to rely on a relatively small number of key frequencies.
2018_07_21_dropout_0.25.out: Dropout was reduced to 25%. Unfortunately, this appeared to be the worst of both worlds. Training accuracy was up around ~99%, and validation accuracy once again peaked at ~91%. It is difficult to know what to make of this; 25% dropout rarely has a detrimental effect this significant.
2018_07_21_l2_0.003.out: For this test, dropout was removed entirely but L2 regularization with a lambda of 0.003 was added to all layers. It is questionable whether or not it alleviated overfitting significantly, since training accuracy peaked at ~99%. Nevertheless, validation accuracy peaked at ~95%, so it was not overly harmful there.
2018_07_21_iterative_cluster_nucleation_full_resolution_without_wall_cuts.out: This was a test of the iterative cluster nucleation system using the new, non-banded Fourier transform training system. This was not successful; multiple major problems were observed. For one, severe overfitting was observed. Validation accuracy was around ~60% for much of the run, while training accuracy reached 100%. Also, as data was added to the training set over the course of the run, a large amount of bad data was introduced. Later in the run, correct additions were outnumbered by incorrect additions. This makes training for reasonable validation accuracy quite impractical.
2018_07_24_iterative_cluster_nucleation_banded_frequency_no_duplication.out: This is a re-test of iterative cluster nucleation, rolled back to the banded frequency training system, with no validation wall cuts. A problem with earlier tests was that each example occurred 3 times in the training set, which greatly reduced the probably number that were actually added. This test started off successfully, adding 53 examples without error after epoch 6. However, it started to go off the rails later on, adding progressively more and more incorrect examples. Also, a discriminator with acceptable validation accuracy was never obtained; overfitting was very significant.
2018_07_24_iterative_cluster_nucleation_with_wall_cuts.out: Validation accuracy was reasonable (but not particularly good) at the beginning, at ~89%. Unfortunately, significant numbers of incorrect examples were added later on. As well, the training accuracy remained very high (~99%) while the validation accuracy dropped due to overfitting.
2018_07_24_iterative_cluster_nucleation_with_dropout_and_l2.out: Validation accuracy was significantly higher during this test, peaking at ~92%. Overfitting was not gona, but was significantly less extreme, with training accuracy not usually more than a few percent above validation accuracy. This indicates that the combination of 50% dropout and L2 regularization with a lambda of 0.003 was a reasonable choice.
2018_07_24_high_resolution_frequency_grid_search.out: This was a reasonably comprehensive automated grid search of network architectures for discrimination between alphas and neutrons, using the full resolution Fourier transform, without any wall cuts. There are a few interesting results from this. One, overfitting was almost universal. The mean training accuracy within a run never went below 92%, while the mean validation accuracy never went above 90%. The maximum validation accuracy for each run was always above 92%, and peaked at ~97%. Admittedly this very high accuracy appeared to be a fluke, since the accuracies around it were below 80%, but there were many occurrences of high accuracy throughout the entire grid search, so it seems reasonable to say that this is likely to occur in any individual run. One interesting and unexpected combination that worked very well was L2 with lambda 0.01 (very high), and no dropout. No matter the number of layers, this produced high mean and maximum validation accuracy. Nevertheless, it was not immune to overfitting, and training accuracy was much higher.
2018_07_26_image_classification_correct_window_cropping.out: This was the first test of the image classification system with the windows correctly cropped out. It was an interesting result: accuracy was not good, but it was not overfitting. Quite the contrary, since training accuracy peaked at 63% while validation peaked at 81%. This is likely due to the use of 50% dropout regularization, which often adversely affects training accuracy. This seems to be overkill.
2018_07_26_image_classification_less_dropout_more_epochs.out: Apparently there are only 84 training examples currently being used. This is less than the 112 seen in the last test, for reasons that are not clear. Naturally, it overfit and got 100% accuracy on the training set. Despite this, the network managed to get 85% accuracy on the validation set.
2018_07_26_time_zero_initial_test.out: This was the initial test of a neural network that is trained on the time zero values for each of the different piezos, to predict the positions of the corresponding bubbles. In this test, it was unable to get the validation loss below exactly 10000. This seemed strange at first, but it is almost certainly because the activation function on the last layer is sigmoid, which is bounded between 0 and 1. Really, it should be unbounded in this case.
2018_07_26_time_zero_no_activation_last_layer.out: This is a rather strange result, to say the least. Training loss seems reasonable, at a mean squared error of 40. That would indicate that it is accurate to between 6mm and 7mm, which is clearly too accurate but not that far off. Validation accuracy reaches a minimum of ~0.06, which is clearly unreasonable for the mean squared error. It is not why this is happening or what it indicates.
2018_07_26_time_zero_no_-100.out: It turned out there was a bug. Rather than removing erroneous position values of (-100, -100, -100), everything else was being removed. Training with this bug fixed was less apparently successful. The validation loss at the end was ~6928, which is a relatively small improvement from the mean squared error of ~10410 when everything is predicted as 0. Examining the validation outputs revealed the cause: an extreme case of underfitting. The network was outputting the exact same value for every validation input.
2018_07_26_time_zero_all_piezos.out: Previously, only the 6 piezos considered "working", and used for the original analysis, were used as inputs. For this test, I included all of them, because the time zero values appeared valid, and also because some of them may be located in different areas of the vessel, while most of the working ones are on the left wall of the cylinder. This should provide additional information from more diverse positions, which should certainly be beneficial for triangulation. It worked somewhat better, reaching a minimum validation loss of ~6240. This was better than the training loss (~6794). Unfortunately, it was once again outputting the same value for every validation example.
2018_07_26_time_zero_subtract_minimum.out: This was a fairly trivial change. The minimum value was subtracted from the timings so the first piezo to receive a signal corresponds to 0. It made performance worse (validation loss was over 7000) but this could simply be noise.
