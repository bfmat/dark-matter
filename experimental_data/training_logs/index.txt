2018_06_27_first_training_run.out: The very first time I trained a neural network on the supercomputer. It was a simple multi-layer neural network trained on the banded Fourier transformed data without AP cuts. It was just a proof of concept.
2018_06_28_first_image_classification.out: A convolutional neural network was trained to distinguish between images from the background run and the americium-beryllium calibration run. Validation data did not work. An unexpected success at ~96% accuracy on the training set; however, I was wary of overfitting because of the very large number of parameters in the first dense layer.
2018_06_28_second_image_classification.out: Very similar to the previous experiment. However, the proportion of bubbles randomly omitted was reduced from 0.8 to 0.5. Also, the number of images held in memory was increased from 256 to 512, and the number of images turned over per batch was increased from 2 to 16, resulting in a greater exposure to different training examples. I added another convolutional layer to reduce the number of parameters in the network. Despite this, it managed to obtain ~97% accuracy on the training set.
2018_06_29_first_audio_classification.out: A one-dimensional convolutional neural network was trained on the raw time domain audio waveforms to distinguish between americium-beryllium calibration runs and background radiation runs. Dropout regularization was included between all layers to alleviate overfitting. It was slow to achieve good performance, but managed 97% accuracy at peak and 93% at the end. This was, once again, unexpectedly high. Validation did not work, as in previous tests with the generator training system.
2018_06_29_first_proper_fiducial_cuts.out: Performance is apparently high, but it seems to be predicting neutron for almost every example. The alphas are so vastly outnumbered that they aren't contributing significantly. The maximum prediction was less than 0.6, so this result is unsurprising.
2018_06_29_improved_fiducial_cuts_with_sigmoid.out: This was a very significant improvement in a way that is not clear in the training performance. Rather than predicting everything to be a neutron, the network is overzealous in classifying as alphas. Many neutron outliers in the louder range of the AP spectrum are errors as such. Almost no alphas are incorrectly classified as neutrons, and most of the ones that are, are extreme outliers. This is, once again, strikingly similar to the graph in the PICO-60 paper.
2018_06_29_image_classification_with_validation.out: Validation set is apparently functional, but its behavior is very strange. It starts off with ~80% accuracy and retains that all the way through the run. Also, the performance in general is much worse than that of the previous image classification run.
2018_06_29_image_classification_with_validation_and_fiducial_cuts.out: This time, the validation score meaningfully changes throughout the training run. However, it is very bad, peaking at around 60%. Training scores are also bad, but significantly better than the validation score.
2018_06_29_image_classification_with_validation_and_height_cuts_only.out: This was done without the standard fiducial cuts but with a "hack" cut that was included in the earlier training runs. It is not entirely clear why this cut made such a large difference, but it did. Rather than not being able to fit at all, it fit with 99.7% accuracy, but only 51% on the validation set (which is worse than random due to the asymmetric class distribution). This clearly answered the question as to whether or not overfitting is the cause for this high accuracy with a resounding yes.
2018_06_30_audio_classification_with_validation.out: This did not include the fiducial cuts at all, even omitting the basic height cut that made the image classification work. Performance was quite good, peaking at ~87% on the training data. Validation performance was confusing like previous tests, starting around 90% and retaining that. It seems like it was not overfitting, but this is not entirely clear based on the strange validation results.
2018_06_30_audio_classification_with_saved_validation_set.out: This peaked at roughly the same performance as the previous run, and had the same parameters except that the validation set was saved at the end. It revealed the cause of the equal validation performance: the network ended up outputting the average of the ground truth values for all of the validation examples. It is not clear why the training performance was lower (below 30%) at the beginning; it may have had to do with the dropout used during training but not validation.
2018_06_30_audio_classification_fully_convolutional_architecture.out: The validation performance oscillated strangely up and down during this run. It peaked at ~96%, but went back down to ~42% even as the training loss continued to decrease and the training performance continued to increase. The last epoch was fairly successful, but not the most in the run. Also, the 99%-100% accuracy in the training dataset was very high relative to previous comparisons. This experiment was done without any fiducial cuts.
2018_06_30_audio_classification_with_saved_models.out: This was an even more notable success. The accuracy remained at 100% for some period of time, and the loss function was extremely low. When the training data was reprented in a graph, it demonstrated almost perfect separation between neutrons and alphas. The validation set was slightly noisier, but not very much; it too was very precise and independent from the AP axis.
2018_07_05_audio_classification_including_all_calibration_sets.out: This was done with all of the neutron calibration data sets combined. It compared unfavorably with previous experiments; training accuracy was fairly high but validation accuracy was between 70% and 80%. It was evidently overfitting much more than was observed with only the americium-beryllium data set.
2018_07_05_audio_classification_with_fiducial_cuts.out: This was done with all of the neutron calibration sets, and with the fiducial cuts enabled. It didn't overfit quite as much as the previous experiment, but it was also not successful. Almost perfect accuracy was seen on the training data, but validation peaked around 90%. The graph proved this relatively high number to be almost meaningless.
