2018_06_27_first_training_run.out: The very first time I trained a neural network on the supercomputer. It was a simple multi-layer neural network trained on the banded Fourier transformed data without AP cuts. It was just a proof of concept.
2018_06_28_first_image_classification.out: A convolutional neural network was trained to distinguish between images from the background run and the americium-beryllium calibration run. Validation data did not work. An unexpected success at ~96% accuracy on the training set; however, I was wary of overfitting because of the very large number of parameters in the first dense layer.
2018_06_28_second_image_classification.out: Very similar to the previous experiment. However, the proportion of bubbles randomly omitted was reduced from 0.8 to 0.5. Also, the number of images held in memory was increased from 256 to 512, and the number of images turned over per batch was increased from 2 to 16, resulting in a greater exposure to different training examples. I added another convolutional layer to reduce the number of parameters in the network. Despite this, it managed to obtain ~97% accuracy on the training set.
2018_06_29_first_audio_classification.out: A one-dimensional convolutional neural network was trained on the raw time domain audio waveforms to distinguish between americium-beryllium calibration runs and background radiation runs. Dropout regularization was included between all layers to alleviate overfitting. It was slow to achieve good performance, but managed 97% accuracy at peak and 93% at the end. This was, once again, unexpectedly high. Validation did not work, as in previous tests with the generator training system.
2018_06_29_first_proper_fiducial_cuts.out: Performance is apparently high, but it seems to be predicting neutron for almost every example. The alphas are so vastly outnumbered that they aren't contributing significantly. The maximum prediction was less than 0.6, so this result is unsurprising.
2018_06_29_improved_fiducial_cuts_with_sigmoid.out: This was a very significant improvement in a way that is not clear in the training performance. Rather than predicting everything to be a neutron, the network is overzealous in classifying as alphas. Many neutron outliers in the louder range of the AP spectrum are errors as such. Almost no alphas are incorrectly classified as neutrons, and most of the ones that are, are extreme outliers. This is, once again, strikingly similar to the graph in the PICO-60 paper.
2018_06_29_image_classification_with_validation.out: Validation set is apparently functional, but its behavior is very strange. It starts off with ~80% accuracy and retains that all the way through the run. Also, the performance in general is much worse than that of the previous image classification run.
2018_06_29_image_classification_with_validation_and_fiducial_cuts.out: This time, the validation score meaningfully changes throughout the training run. However, it is very bad, peaking at around 60%. Training scores are also bad, but significantly better than the validation score.
2018_06_29_image_classification_with_validation_and_height_cuts_only.out: This was done without the standard fiducial cuts but with a "hack" cut that was included in the earlier training runs. It is not entirely clear why this cut made such a large difference, but it did. Rather than not being able to fit at all, it fit with 99.7% accuracy, but only 51% on the validation set (which is worse than random due to the asymmetric class distribution). This clearly answered the question as to whether or not overfitting is the cause for this high accuracy with a resounding yes.
2018_06_30_audio_classification_with_validation.out: This did not include the fiducial cuts at all, even omitting the basic height cut that made the image classification work. Performance was quite good, peaking at ~87% on the training data. Validation performance was confusing like previous tests, starting around 90% and retaining that. It seems like it was not overfitting, but this is not entirely clear based on the strange validation results.
2018_06_30_audio_classification_with_saved_validation_set.out: This peaked at roughly the same performance as the previous run, and had the same parameters except that the validation set was saved at the end. It revealed the cause of the equal validation performance: the network ended up outputting the average of the ground truth values for all of the validation examples. It is not clear why the training performance was lower (below 30%) at the beginning; it may have had to do with the dropout used during training but not validation.
2018_06_30_audio_classification_fully_convolutional_architecture.out: The validation performance oscillated strangely up and down during this run. It peaked at ~96%, but went back down to ~42% even as the training loss continued to decrease and the training performance continued to increase. The last epoch was fairly successful, but not the most in the run. Also, the 99%-100% accuracy in the training dataset was very high relative to previous comparisons. This experiment was done without any fiducial cuts.
2018_06_30_audio_classification_with_saved_models.out: This was an even more notable success. The accuracy remained at 100% for some period of time, and the loss function was extremely low. When the training data was reprented in a graph, it demonstrated almost perfect separation between neutrons and alphas. The validation set was slightly noisier, but not very much; it too was very precise and independent from the AP axis.
2018_07_05_audio_classification_including_all_calibration_sets.out: This was done with all of the neutron calibration data sets combined. It compared unfavorably with previous experiments; training accuracy was fairly high but validation accuracy was between 70% and 80%. It was evidently overfitting much more than was observed with only the americium-beryllium data set.
2018_07_05_audio_classification_with_fiducial_cuts.out: This was done with all of the neutron calibration sets, and with the fiducial cuts enabled. It didn't overfit quite as much as the previous experiment, but it was also not successful. Almost perfect accuracy was seen on the training data, but validation peaked around 90%. The graph proved this relatively high number to be almost meaningless.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy.out: This was the same as the previous test, except that class weights were added so the alphas were weighted 8 times as heavily as the neutrons, and the loss function was changed from mean squared error to binary crossentropy. The loss function was not good; validation varied widely from 80%-90% down to 40%, sometimes between just one epoch. Training was very accurate, at ~99%.
2018_07_05_audio_classification_with_mean_squared_error.out: This was same as the previous run, except that the binary crossentropy loss function was changed back to mean squared error. The class weights were left in place. Performance was very similar, demonstrating high training accuracy but poor validation accuracy (~85%). Overfitting was to a very similar degree.
2018_07_06_audio_classification_with_only_californium_60cm.out: The network architecture was reverted back to the same as the one that trained near-perfectly on the americium-beryllium calibration set. Instead, it was trained to distinguish between low background data and californium at 60cm. Performance was, unfortunately, very similar to the tests with mixed datasets. Training accuracy was high (~99%), but validation accuracy was very low (sub-70%).
2018_07_06_audio_classification_with_only_barium_40cm.out: This was the same as the previous run, with californium at 60cm switched out for barium at 40cm. It had a very similar training result: training performance was very high as usual, but validation peaked at ~64%. This makes it increasingly clear that the americium data set is unusual in that there is a clear giveaway that separates it from the calibration run without any corrections, given that neither californium at 60cm nor barium at 40cm demonstrated this property.
2018_07_06_audio_classification_americium_without_pressure_cut.out: This was a test of the original system used on the americium data, trained on the americium data. The difference was that the pressure cuts to determine the number of bubbles were removed. The resulting accuracy was very strange. Training accuracy was very high, as expected. Validation accuracy reached ~98% in the earlier epochs, but then dropped to ~35%. For the last epoch, it achieved 99.2%, after remaining at a very low accuracy for several epochs, including the one immediately before. This is notably higher than the best validation accuracy achieved in previous americium tests. It is not at all clear what changed in the training process to make it jump up so sporadically.
2018_07_06_audio_classification_all_classes_very_deep_architecture.out: This was a test of all of the classes without weighting, using a very deep architecture based on M34 (which was designed from the ground up for processing of raw audio waveforms). It did not include even a single dense layer; the final output was the result of a global average pooling layer. It appeared not to make a great difference to validation performance; it jumped between ~71% and ~31%. Training accuracy was, as always, above 99%. However, the results of the last test brought into question the integrity of the data set used to train this as well as previous models.
2018_07_07_audio_classification_all_classes_very_deep_with_dense.out: The architecture used for this test was something of a balance between the original model with relatively few layers and the extreme M34-inspired architecture. The size of the max pooling layers was increased from 6 to 8 (to reduce the number of outputs on the final convolutional layer) and the average pooling was replaced with a single dense layer. Training performance was actually more limited (around 97%). Validation performance oscillated quite significantly, peaking around 91%.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01.out: The regularization used here in an attempt to alleviate the overfitting observed in the previous test seems to have been overkill; the training accuracy was low (less than 92% at all times) and so was the validation accuracy. In addition, the validation accuracy still oscillated widely like during previous experiments.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001.out: The performance for this run was very similar to what it was without regularization, except that the training data performance was somewhat lower (~94% rather than ~97%). It had a very similar maximum performance, as well; around 90%. It reached around this performance on a few different epochs rather than just one. However, it is still very unstable and changes a lot between epochs.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003.out: This run performed arguably better than any of the previous ones; its peak accuracy was ~91%, and it managed to maintain approximately this for more than one epoch, as well as reaching that peak twice. It was still sporadic, and no model tested so far solved this problem. Also, it is beginning to look like there is a practical limit of ~91% accuracy on validation using this data set. This appears to be a reasonably moderate value for the regularization lambda parameter.
2018_07_08_first_iterative_cluster_nucleation.out: It is not clear what the problem is in this instance. On the first epoch, the network achieves a training accuracy of 95%. After that, as it adds more data to the training set, its performance degrades and hovers around ~50% for the rest of the experimental run. It is possible that the high-confidence examples are essentially random after such a short training period, and more time (and potentially regularization) is required for a meaningful result.
2018_07_09_audio_classification_with_zero_image_bubble_events.out: This run did not change any network or training parameters versus previous audio classification trials. A problem with the multi-bubble data cut was fixed; there were many events with 0 bubbles recognized by the camera which were cut since there was apparently not 1 bubble. It was changed to include events with 0 or 1 bubbles, provided the pressure transducer cut also accepted them as having approximately 1 bubble. As always, training was sporadic, but the peak of validation performance here was higher than ever before, at ~93%.
2018_07_09_audio_classification_with_normalized_recordings.out: This changes nothing from the previous run, except that the recordings are normalized to have a geometric mean (standard deviation off of 0) equal to 1. Based on the accuracy statistics, this seems to not have had any ill effect. None were expected, but this was important to confirm before adding any kind of noise or other transformations.
2018_07_09_audio_classification_with_cropped_inputs.out: For this run, the audio recordings were cropped, removing the first 90000 samples (which are noise at the beginning) and the last 60000 (which are very loud recordings of the hydraulic system re-pressurizing the vessel after a bubble forms). There was evidently some information in the data that has been removed that could be used to discriminate. However, it is not data that should be provided, and any accuracy that relies on this data amounts to overfitting. Performance in this run was considerably worse; training accuracy peaked at ~88%, and validation at ~81%. It is possible the regularization used to prevent overfitting when accuracy was high is hindering performance. Interestingly, validation accuracy was significantly less sporadic in this run. It still oscillated, and in fact got quite significantly worse at one point during the middle of the test, but these were slower, more gradual changes as compared to the rapid and extreme oscillations observed in previous tests.
2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization: This run had regularization disabled, to see if it alleviated the lack of fitting observed in the previous test. It seems it may have improved the situation somewhat, with the validation accuracy peaking at ~86% (and this exact accuracy maintained for two epochs, 16 and 17). Notably, it does not seem to be overfitting appreciably; the training accuracy corresponding to that peak was ~88%.
2018_07_09_iterative_cluster_nucleation_multiple_epochs_per_iteration.out: This was a test of a mostly identical iterative cluster nucleation system. Choices for further training data were apparently poor in the first test, so multiple epochs were run before selection of further training data during this test. Unfortunately, it did not appreciably improve the section. Training accuracy during the first iteration settled at ~95%, but this figure dropped very far during the second and later iterations (always below 70%). The cause of this is unclear at the moment; there may be some issues with the training data selection system.
2018_07_09_audio_classification_with_residual_blocks_on_every_convolutional_layer.out: The model for this test had a residual block around every convolutional layer, meaning that the input and the output of the layer are added together. This is not entirely faithful to the original M34-res architecture, but was an attempted proof of concept. However, it was a failure. Validation was very stable, mostly remaining between 70% and 80% with slow, gradual oscillations. There were no peaks outside of this range. Training accuracy was higher, but not very much; it never reached even 85%. It is possible the residual blocks around every layer are detrimental, and they should instead be around every two layers as in the original paper.
2018_07_10_audio_classification_with_more_layers_and_residual_blocks_on_every_two.out: This was a disaster. The network had almost twice as many convolutional layers, and residual blocks on every two layers, as in the M34-res architecture. Training accuracy never reached even 75%, and validation accuracy peaked at 78% very early in the run. Several epochs had a validation accuracy below 40%. It was also not stable like the previous test; there were very significant oscillations.
2018_07_10_audio_classification_with_more_layers_and_no_residual_blocks.out: The poor performance on both runs with residual blocks raised questions about whether or not they are effective at all for this task. This test used the same architecture as the last run (with almost twice as many layers as previous runs) but without any residual blocks. It was yet another mediocre performance. Training accuracy hovered around 70%-75%, and validation accuracy peaked at ~73%. It is noteworthy that this performed worse than a model with around half as many layers and no particular features that should give it an edge (2018_07_09_audio_classification_with_cropped_inputs_and_no_regularization).
2018_07_10_audio_classification_validation_with_fiducial_cuts.out: This was the same as the recent relatively successful run, except that validation was judged strictly based on the data including fiducial cuts. This gives a more direct comparison against the accuracy of AP for discrimination. Training accuracy was consistently high at ~96% for the last few epochs, which is curiously higher than the last time this system was tested. Validation performance was fairly stable around 90% towards the ending, which is notably better than was obtained with the original full data set. There is clearly some degree of overfitting; it is possible re-introducing regularization is necessary.
2018_07_10_audio_classification_trained_with_fiducial_cuts.out: This was an attempt to determine whether or not training on the rest of the data improved performance on the data after fiducial cuts. There was reason to be skeptical of the benefits of training on the wall events, because the acoustics were said to be so different. Surprisingly, the training accuracy was worse (~93% rather than ~96%). Validation performance was similar (~91%) but less consistent, and had a lower consistent accuracy. At the moment, there seem to be no benefits to training only on data after fiducial cuts.
2018_07_10_initial_high_resolution_frequency_domain.out: This initial test of the frequency domain training system was a failure for reasons that are not entirely clear. A Fourier transform of the audio data, divided into 16 time bands and 1024 frequency bands, was flattened and fed into a dense neural network. The validation accuracy stabilized around 68% before diving to ~32% at the very end, and the training accuracy was notably worse (around 50% for much of the run).
2018_07_10_audio_classification_with_secondary_position_input.out: This was a test of a modified convolutional network architecture, where the position of the bubble was concatenated with the outputs of the last convolutional layer, to be input into the first dense layer. The objective was to simulate the positional corrections used to compute the AP, within the neural network. Normalization was disabled so that magnitude (combined with position corrections) could be hopefully taken into account. Training accuracy was reasonably high (~97%) but validation accuracy was consistently below 70%. This seems like a textbook case of overfitting.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.0003.out: This introduced L2 regularization to the same setup as the previous test. Surprisingly, its validation accuracy was still much worse than before the additional input was introduced, but better than before any regularization was introduced. It is apparently still overfitting on the new inputs. Training accuracy is somewhat better than before the new input was introduced (~94%, up from ~88%), but this represents nothing other than overfitting.
2018_07_10_audio_classification_with_secondary_position_input_and_l2_0.001.out: The increased regularization did nothing to alleviate the overfitting. Validation accuracy peaked at ~75%, and training accuracy was very similar to what it was before. Its accuracy was very stable, but not good, oscillating generally between 65% and 75%. For experiments in the immediate future, the regularization lambda parameter will be decreased back to 0.0003 because of how the larger parameter caused underfitting problems the last time it was tested.
2018_07_12_audio_classification_with_l2_0.0003_and_no_barium_data.out: For this experiment, the regularization lambda was reverted to its original value. Also, the barium data sets were removed from the neutron class. They are gamma calibration sets, not neutron, so they are invalid for the context of discriminating between alphas and neutrons. The result was slightly better but not nearly enough. Validation accuracy peaked at ~80%, and was not overly stable (it oscillated around the range of ~65% to ~80%). Training accuracy demonstrated overfitting; at the peak validation epoch, it was ~92%, and it went up from there later on.
2018_07_16_banded_frequency_new_validation_cuts.out: This is the same network used during the very early tests with the banded frequency domain data. Surprisingly, its performance is quite poor. Training accuracy peaks at ~89%, and validation at ~88%. Particularly for validation, this number is unstable, where it frequently dipped below 80%, even near the end of training.
2018_07_16_banded_frequency_smaller_architecture.out: This was a slight improvement, with one layer removed and the other reduced to 12 neurons. Validation accuracy peaked at ~89%, and did not overfit substantially.
2018_07_16_banded_frequency_validation_not_removed.out: There was a significant bug where, rather than just the validation data being removed from the training set as was intended, all data which passed the validation tests (most of which were supposed to be used for training) were removed from the training data. This made a very significant difference to the result: validation accuracy peaked at 96%, which was significantly higher than the ~88% training accuracy present for much of the training run.
2018_07_16_banded_frequency_training_data_passes_validation_cuts.out: This apparently now does not have any disadvantages as compared to AP. Its accuracy for binary classification is slightly higher, and its standard deviation for both classes is lower. Interestingly, the validation accuracy is slightly lower after training on only data that passes the validation tests (although this could just be a case of a slightly more difficult randomly selected validation set).
2018_07_16_banded_frequency_linear_regression.out: This is still trained only on data that passes the validation cuts. The linear regression model has a lower validation accuracy than the previous tests (~92%), although it is remarkably stable. Training accuracy stayed reliably above 90%, frequently with a noticeably higher accuracy than the validation data. This is interesting, with such a simple model.
2018_07_16_banded_frequency_validation_passing_weighted_3_times.out: This is a reversion to the neural network model. It is no longer just trained on the data that passes validation tests. It includes all data that passes the standard tests, but the examples which pass the validation tests are weighted 3 times as heavily for training purposes. This is an improvement in absolute accuracy; it peaks at ~97% and is somewhat stable at ~96%. The training accuracy never stabilizes above 90%, so it is still clear that the realities of the training data is not unduly dragging down the accuracy on the validation data.
2018_07_16_banded_frequency_weight_reduced_to_2_times.out: The weight of the data which passes validation tests has been reduced to 2 times that of normal data. The validation accuracy is stable, but at a much lower level (93%). It seems that the greater weight has a positive impact on the network's validation accuracy. Also notably, the training accuracy seems to be minimally affected by all of these changes, remaining around 89% much of the time.
2018_07_16_waveform_with_new_cuts.out: This was a sanity test of the waveform training system. A lot was changed since it had last been experimented with. It is increasingly seeming that the overall volume of the event is a necessary parameter for discrimination, and that the normalized (or non-corrected) frequency distribution is not sufficient, once the main causes of overfitting have been removed and the validation data has been cut to the segment where AP is an accurate discriminator. The results from this training run are in line with these expectations. Training accuracy is suboptimal, remaining mostly between 70% and 80%. The validation accuracy is in a similar range. This does not signify any kind of meaningful accuracy. This network does not have a positional input, so it would be unable to perform any kind of internal corrections. Therefore, it is unexpected that high accuracy would be obtained. In a recent observation, it was determined that by listening to the audio recordings, a human can hear the difference between alpha particles and neutrons. However, it is highly dependent on the loudness of the initial sound. The neural network is likely similar.
