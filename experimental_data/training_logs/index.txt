2018_06_27_first_training_run.out: The very first time I trained a neural network on the supercomputer. It was a simple multi-layer neural network trained on the banded Fourier transformed data without AP cuts. It was just a proof of concept.
2018_06_28_first_image_classification.out: A convolutional neural network was trained to distinguish between images from the background run and the americium-beryllium calibration run. Validation data did not work. An unexpected success at ~96% accuracy on the training set; however, I was wary of overfitting because of the very large number of parameters in the first dense layer.
2018_06_28_second_image_classification.out: Very similar to the previous experiment. However, the proportion of bubbles randomly omitted was reduced from 0.8 to 0.5. Also, the number of images held in memory was increased from 256 to 512, and the number of images turned over per batch was increased from 2 to 16, resulting in a greater exposure to different training examples. I added another convolutional layer to reduce the number of parameters in the network. Despite this, it managed to obtain ~97% accuracy on the training set.
2018_06_29_first_audio_classification.out: A one-dimensional convolutional neural network was trained on the raw time domain audio waveforms to distinguish between americium-beryllium calibration runs and background radiation runs. Dropout regularization was included between all layers to alleviate overfitting. It was slow to achieve good performance, but managed 97% accuracy at peak and 93% at the end. This was, once again, unexpectedly high. Validation did not work, as in previous tests with the generator training system.
2018_06_29_first_proper_fiducial_cuts.out: Performance is apparently high, but it seems to be predicting neutron for almost every example. The alphas are so vastly outnumbered that they aren't contributing significantly. The maximum prediction was less than 0.6, so this result is unsurprising.
2018_06_29_improved_fiducial_cuts_with_sigmoid.out: This was a very significant improvement in a way that is not clear in the training performance. Rather than predicting everything to be a neutron, the network is overzealous in classifying as alphas. Many neutron outliers in the louder range of the AP spectrum are errors as such. Almost no alphas are incorrectly classified as neutrons, and most of the ones that are, are extreme outliers. This is, once again, strikingly similar to the graph in the PICO-60 paper.
2018_06_29_image_classification_with_validation.out: Validation set is apparently functional, but its behavior is very strange. It starts off with ~80% accuracy and retains that all the way through the run. Also, the performance in general is much worse than that of the previous image classification run.
2018_06_29_image_classification_with_validation_and_fiducial_cuts.out: This time, the validation score meaningfully changes throughout the training run. However, it is very bad, peaking at around 60%. Training scores are also bad, but significantly better than the validation score.
2018_06_29_image_classification_with_validation_and_height_cuts_only.out: This was done without the standard fiducial cuts but with a "hack" cut that was included in the earlier training runs. It is not entirely clear why this cut made such a large difference, but it did. Rather than not being able to fit at all, it fit with 99.7% accuracy, but only 51% on the validation set (which is worse than random due to the asymmetric class distribution). This clearly answered the question as to whether or not overfitting is the cause for this high accuracy with a resounding yes.
2018_06_30_audio_classification_with_validation.out: This did not include the fiducial cuts at all, even omitting the basic height cut that made the image classification work. Performance was quite good, peaking at ~87% on the training data. Validation performance was confusing like previous tests, starting around 90% and retaining that. It seems like it was not overfitting, but this is not entirely clear based on the strange validation results.
2018_06_30_audio_classification_with_saved_validation_set.out: This peaked at roughly the same performance as the previous run, and had the same parameters except that the validation set was saved at the end. It revealed the cause of the equal validation performance: the network ended up outputting the average of the ground truth values for all of the validation examples. It is not clear why the training performance was lower (below 30%) at the beginning; it may have had to do with the dropout used during training but not validation.
2018_06_30_audio_classification_fully_convolutional_architecture.out: The validation performance oscillated strangely up and down during this run. It peaked at ~96%, but went back down to ~42% even as the training loss continued to decrease and the training performance continued to increase. The last epoch was fairly successful, but not the most in the run. Also, the 99%-100% accuracy in the training dataset was very high relative to previous comparisons. This experiment was done without any fiducial cuts.
2018_06_30_audio_classification_with_saved_models.out: This was an even more notable success. The accuracy remained at 100% for some period of time, and the loss function was extremely low. When the training data was reprented in a graph, it demonstrated almost perfect separation between neutrons and alphas. The validation set was slightly noisier, but not very much; it too was very precise and independent from the AP axis.
2018_07_05_audio_classification_including_all_calibration_sets.out: This was done with all of the neutron calibration data sets combined. It compared unfavorably with previous experiments; training accuracy was fairly high but validation accuracy was between 70% and 80%. It was evidently overfitting much more than was observed with only the americium-beryllium data set.
2018_07_05_audio_classification_with_fiducial_cuts.out: This was done with all of the neutron calibration sets, and with the fiducial cuts enabled. It didn't overfit quite as much as the previous experiment, but it was also not successful. Almost perfect accuracy was seen on the training data, but validation peaked around 90%. The graph proved this relatively high number to be almost meaningless.
2018_07_05_audio_classification_with_class_weights_and_binary_crossentropy.out: This was the same as the previous test, except that class weights were added so the alphas were weighted 8 times as heavily as the neutrons, and the loss function was changed from mean squared error to binary crossentropy. The loss function was not good; validation varied widely from 80%-90% down to 40%, sometimes between just one epoch. Training was very accurate, at ~99%.
2018_07_05_audio_classification_with_mean_squared_error.out: This was same as the previous run, except that the binary crossentropy loss function was changed back to mean squared error. The class weights were left in place. Performance was very similar, demonstrating high training accuracy but poor validation accuracy (~85%). Overfitting was to a very similar degree.
2018_07_06_audio_classification_with_only_californium_60cm.out: The network architecture was reverted back to the same as the one that trained near-perfectly on the americium-beryllium calibration set. Instead, it was trained to distinguish between low background data and californium at 60cm. Performance was, unfortunately, very similar to the tests with mixed datasets. Training accuracy was high (~99%), but validation accuracy was very low (sub-70%).
2018_07_06_audio_classification_with_only_barium_40cm.out: This was the same as the previous run, with californium at 60cm switched out for barium at 40cm. It had a very similar training result: training performance was very high as usual, but validation peaked at ~64%. This makes it increasingly clear that the americium data set is unusual in that there is a clear giveaway that separates it from the calibration run without any corrections, given that neither californium at 60cm nor barium at 40cm demonstrated this property.
2018_07_06_audio_classification_americium_without_pressure_cut.out: This was a test of the original system used on the americium data, trained on the americium data. The difference was that the pressure cuts to determine the number of bubbles were removed. The resulting accuracy was very strange. Training accuracy was very high, as expected. Validation accuracy reached ~98% in the earlier epochs, but then dropped to ~35%. For the last epoch, it achieved 99.2%, after remaining at a very low accuracy for several epochs, including the one immediately before. This is notably higher than the best validation accuracy achieved in previous americium tests. It is not at all clear what changed in the training process to make it jump up so sporadically.
2018_07_06_audio_classification_all_classes_very_deep_architecture.out: This was a test of all of the classes without weighting, using a very deep architecture based on M34 (which was designed from the ground up for processing of raw audio waveforms). It did not include even a single dense layer; the final output was the result of a global average pooling layer. It appeared not to make a great difference to validation performance; it jumped between ~71% and ~31%. Training accuracy was, as always, above 99%. However, the results of the last test brought into question the integrity of the data set used to train this as well as previous models.
2018_07_07_audio_classification_all_classes_very_deep_with_dense.out: The architecture used for this test was something of a balance between the original model with relatively few layers and the extreme M34-inspired architecture. The size of the max pooling layers was increased from 6 to 8 (to reduce the number of outputs on the final convolutional layer) and the average pooling was replaced with a single dense layer. Training performance was actually more limited (around 97%). Validation performance oscillated quite significantly, peaking around 91%.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.01.out: The regularization used here in an attempt to alleviate the overfitting observed in the previous test seems to have been overkill; the training accuracy was low (less than 92% at all times) and so was the validation accuracy. In addition, the validation accuracy still oscillated widely like during previous experiments.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.001.out: The performance for this run was very similar to what it was without regularization, except that the training data performance was somewhat lower (~94% rather than ~97%). It had a very similar maximum performance, as well; around 90%. It reached around this preformance on a few different epochs rather than just one. However, it is still very unstable and changes a lot between epochs.
2018_07_07_audio_classification_all_classes_very_deep_with_l2_0.0003.out: This run performed arguably better than any of the previous ones; its peak accuracy was ~91%, and it managed to maintain approximately this for more than one epoch, as well as reaching that peak twice. It was still sporadic, and no model tested so far solved this problem. Also, it is beginning to look like there is a practical limit of ~91% accuracy on validation using this data set. This appears to be a reasonably moderate value for the regularization lambda parameter.
2018_07_08_first_iterative_cluster_nucleation.out: It is not clear what the problem is in this instance. On the first epoch, the network achieves a training accuracy of 95%. After that, as it adds more data to the training set, its performance degrades and hovers around ~50% for the rest of the experimental run. It is possible that the high-confidence examples are essentially random after such a short training period, and more time (and potentially regularization) is required for a meaningful result.
2018_07_09_audio_classification_with_zero_image_bubble_events.out: This run did not change any network or training parameters versus previous audio classification trials. A problem with the multi-bubble data cut was fixed; there were many events with 0 bubbles recognized by the camera which were cut since there was apparently not 1 bubble. It was changed to include events with 0 or 1 bubbles, provided the pressure transducer cut also accepted them as having approximately 1 bubble. As always, training was sporadic, but the peak of validation performance here was higher than ever before, at ~93%.
2018_07_09_audio_classification_with_normalized_recordings.out: This changes nothing from the previous run, except that the recordings are normalized to have a geometric mean (standard deviation off of 0) equal to 1. Based on the accuracy statistics, this seems to not have had any ill effect. None were expected, but this was important to confirm before adding any kind of noise or other transformations.
2018_07_09_audio_classification_with_cropped_inputs.out: For this run, the audio recordings were cropped, removing the first 90000 samples (which are noise at the beginning) and the last 60000 (which are very loud recordings of the hydraulic system re-pressurizing the vessel after a bubble forms). There was evidently some information in the data that has been removed that could be used to discriminate. However, it is not data that should be provided, and any accuracy that relies on this data amounts to overfitting. Performance in this run was considerably worse; training accuracy peaked at ~88%, and validation at ~81%. It is possible the regularization used to prevent overfitting when accuracy was high is hindering performance. Interestingly, validation accuracy was significantly less sporadic in this run. It still oscillated, and in fact got quite significantly worse at one point during the middle of the test, but these were slower, more gradual changes as compared to the rapid and extreme oscillations observed in previous tests.
