2018_06_27_first_training_run.out: The very first time I trained a neural network on the supercomputer. It was a simple multi-layer neural network trained on the banded Fourier transformed data without AP cuts. It was just a proof of concept.
2018_06_28_first_image_classification.out: A convolutional neural network was trained to distinguish between images from the background run and the americium-beryllium calibration run. Validation data did not work. An unexpected success at ~96% accuracy on the training set; however, I was wary of overfitting because of the very large number of parameters in the first dense layer.
2018_06_28_second_image_classification.out: Very similar to the previous experiment. However, the proportion of bubbles randomly omitted was reduced from 0.8 to 0.5. Also, the number of images held in memory was increased from 256 to 512, and the number of images turned over per batch was increased from 2 to 16, resulting in a greater exposure to different training examples. I added another convolutional layer to reduce the number of parameters in the network. Despite this, it managed to obtain ~97% accuracy on the training set.
2018_06_29_first_audio_classification.out: A one-dimensional convolutional neural network was trained on the raw time domain audio waveforms to distinguish between americium-beryllium calibration runs and background radiation runs. Dropout regularization was included between all layers to alleviate overfitting. It was slow to achieve good performance, but managed 97% accuracy at peak and 93% at the end. This was, once again, unexpectedly high. Validation did not work, as in previous tests with the generator training system.
2018_06_29_first_proper_fiducial_cuts.out: Performance is apparently high, but it seems to be predicting neutron for almost every example. The alphas are so vastly outnumbered that they aren't contributing significantly. The maximum prediction was less than 0.6, so this result is unsurprising.
2018_06_29_improved_fiducial_cuts_with_sigmoid.out: This was a very significant improvement in a way that is not clear in the training performance. Rather than predicting everything to be a neutron, the network is overzealous in classifying as alphas. Many neutron outliers in the louder range of the AP spectrum are errors as such. Almost no alphas are incorrectly classified as neutrons, and most of the ones that are, are extreme outliers. This is, once again, strikingly similar to the graph in the PICO-60 paper.
2018_06_29_image_classification_with_validation.out: Validation set is apparently functional, but its behavior is very strange. It starts off with ~80% accuracy and retains that all the way through the run. Also, the performance in general is much worse than that of the previous image classification run.
2018_06_29_image_classification_with_validation_and_fiducial_cuts.out: This time, the validation score meaningfully changes throughout the training run. However, it is very bad, peaking at around 60%. Training scores are also bad, but significantly better than the validation score.
